"""
Advanced Variant Detection for Vietnamese Toxic Content
- Homoglyph detection: detect similar Unicode characters
- Leetspeak normalization: normalize leetspeak
- Insertion attack detection: detect special character insertion
- Pattern-based obfuscation detection: detect complex obfuscation

Version: 1.0.0
Last Updated: 2025-12-19
"""

import re
from typing import Dict, List, Tuple, Set, Any
import unicodedata
import logging

logger = logging.getLogger(__name__)


HOMOGLYPHS = {
    # Latin lookalikes
    '–∞': 'a',  # Cyrillic
    '–µ': 'e',  # Cyrillic
    '—ñ': 'i',  # Cyrillic
    '–æ': 'o',  # Cyrillic
    '—Ä': 'p',  # Cyrillic
    '—Å': 'c',  # Cyrillic
    '‚Ö∞': 'i',  # Roman numeral
    '‚Ö±': 'ii',
    '‚Ö≤': 'iii',
    '‚Ö≥': 'iv',
    '‚Ö¥': 'v',
    '‚Öµ': 'vi',
    '‚Ö∂': 'vii',
    '‚Ö∑': 'viii',
    '‚Ö∏': 'ix',
    '‚Öπ': 'x',
    
    # Full-width characters
    'ÔΩÅ': 'a', 'ÔΩÇ': 'b', 'ÔΩÉ': 'c', 'ÔΩÑ': 'd', 'ÔΩÖ': 'e',
    'ÔΩÜ': 'f', 'ÔΩá': 'g', 'ÔΩà': 'h', 'ÔΩâ': 'i', 'ÔΩä': 'j',
    'ÔΩã': 'k', 'ÔΩå': 'l', 'ÔΩç': 'm', 'ÔΩé': 'n', 'ÔΩè': 'o',
    'ÔΩê': 'p', 'ÔΩë': 'q', 'ÔΩí': 'r', 'ÔΩì': 's', 'ÔΩî': 't',
    'ÔΩï': 'u', 'ÔΩñ': 'v', 'ÔΩó': 'w', 'ÔΩò': 'x', 'ÔΩô': 'y', 'ÔΩö': 'z',
    
    # Special symbols
    '@': 'a', '4': 'a', '‚àÇ': 'a',
    '3': 'e', '‚Ç¨': 'e', '√´': 'e', '√™': 'e',
    '1': 'i', '!': 'i', '|': 'i', '√Ø': 'i', '√Æ': 'i',
    '0': 'o', '√∏': 'o', '√¥': 'o', '√∂': 'o',
    '5': 's', '$': 's', '≈°': 's',
    '7': 't', '+': 't',
    '√º': 'u', '√ª': 'u', '√∫': 'u', '√π': 'u',
    '¬•': 'y', '√ø': 'y',
    '2': 'z',
    
    # Vietnamese specific homoglyphs
    'ƒë': 'd', 'ƒê': 'd',
    '√∞': 'd',  # Icelandic eth
    
    # Math symbols that look like letters
    '√ó': 'x',
    '√∑': 't',
    '‚àû': 'oo',
}

# ==================== LEETSPEAK MAPPING ====================

LEETSPEAK_TO_LETTER = {
    # Numbers to letters
    '0': 'o',
    '1': 'i',
    '2': 'z',
    '3': 'e',
    '4': 'a',
    '5': 's',
    '6': 'g',
    '7': 't',
    '8': 'b',
    '9': 'g',
    
    # Symbols
    '@': 'a',
    '!': 'i',
    '$': 's',
    '+': 't',
    '(': 'c',
    ')': 'c',
    '[': 'c',
    ']': 'c',
    '|': 'i',
    '\\': 'l',
    '/': 'l',
    '^': 'a',
    '<': 'c',
    '>': 'c',
    '{': 'c',
    '}': 'c',
    '~': 'n',
    '*': 'x',
    '#': 'h',
    '%': 'x',
    '&': 'and',
}

# ==================== INSERTION PATTERNS ====================
# Patterns for special characters inserted between letters

INSERTION_CHARS = [
    '.', '-', '_', ' ', '*', '~', '^', "'", '"',
    '`', '|', '/', '\\', '+', '=', '#', '@',
    '‚Ä¢', '¬∑', '¬∞', '‚ó¶', '‚óã', '‚óè', '‚óØ', '‚òÖ', '‚òÜ',
    '‚ô°', '‚ô•', '‚ù§', 'üíï', 'üî•', '‚ú®', 'üíØ',
]

# ==================== VIETNAMESE TOXIC VARIANTS ====================

# Variants of Vietnamese toxic words
TOXIC_VARIANTS = {
    # ===== ƒê·ª§/ƒê·ªäT family =====
    'du': {
        'normalized': 'ƒë·ª•',
        'variants': [
            # ONLY match EXACT obfuscation patterns - DO NOT match standalone 'ƒë·ª•', 'd·ª•'
            'ƒëut', 'dut', 'ƒë·ª•t', 'd·ª•t',
            'ƒë.u', 'd.u', 'ƒë_u', 'd_u', 'ƒë-u', 'd-u',
            'ƒë u', 'd u', 'ƒë  u', 'd  u',
            'ƒë*u', 'd*u', 'ƒë@u', 'd@u',
            # Leetspeak
            'du7', 'd07', 'dv', 'ƒëv',
            # Unicode
            '…ó·ª•', '…óu', 'ƒë¬µ', 'dŒº',
            # NOTE: 'ƒë·ª•', 'd·ª•', 'ƒëu', 'd√π', 'd∆∞', 'd·ª±' REMOVED - too common
        ],
        'severity': 'high',
        'require_word_boundary': True,  # ONLY detect when word stands alone
        # CRITICAL: Safe contexts - EXPANDED to reduce false positives
        'safe_contexts': [
            # ===== Du l·ªãch family =====
            'du l·ªãch', 'du h·ªçc', 'du kh√°ch', 'du xu√¢n', 'du h√†nh',
            'du thuy·ªÅn', 'du ngo·∫°n', 'du ca', 'h∆∞·ªõng d·∫´n du',
            'kh√°ch du', 'tour du', 'chuy·∫øn du', 'ƒëi du', 'c√¥ng ty du',
            
            # ===== Duy√™n family - VERY IMPORTANT =====
            'duy√™n', 'duy√™n d√°ng', 'duy√™n ph·∫≠n', 'duy√™n n·ª£', 'duy√™n s·ªë',
            'c√≥ duy√™n', 'h·ªØu duy√™n', 'v√¥ duy√™n', 'nh√¢n duy√™n', 't√¨nh duy√™n',
            'duy√™n h·∫£i', 'duy√™n do', 'duy√™n c·ªõ',
            
            # ===== Duy·ªát family - VERY COMMON =====
            'duy·ªát', 'ki·ªÉm duy·ªát', 'ph√™ duy·ªát', 'x√©t duy·ªát', 'th·∫©m duy·ªát',
            'duy·ªát binh', 'duy·ªát x√©t', 'ƒë∆∞·ª£c duy·ªát', 'ch·ªù duy·ªát',
            'n·ªôi dung duy·ªát', 'h·ªá th·ªëng duy·ªát', 't·ª± ƒë·ªông duy·ªát',
            
            # ===== D·ª•/D·ª•ng family =====
            'v√≠ d·ª•', 'd·ª•ng c·ª•', 's·ª≠ d·ª•ng', 't√°c d·ª•ng', 'c√¥ng d·ª•ng', '·ª©ng d·ª•ng',
            'd·ª• d·ªó', 'd·ª•ng', 'thi·∫øt b·ªã', 'ph·ª• dung', 'dung l∆∞·ª£ng', 'dung m√¥i',
            'dung d·ªã', 'dung nham', 'dung t√∫ng', 'bao dung',
            
            # ===== D∆∞/D·ªØ family =====
            'd∆∞ th·ª´a', 'd∆∞ gi·∫£', 'd∆∞ lu·∫≠n', 'c√≤n d∆∞', 'd∆∞ √¢m', 'th·∫∑ng d∆∞',
            'd·ªØ li·ªáu', 'd·ªØ d·ªôi', 'd·ªØ ki·ªán', 'c∆° s·ªü d·ªØ', 'l∆∞u tr·ªØ d·ªØ',
            
            # ===== D·ª± family =====
            'd·ª± √°n', 'd·ª± b√°o', 'd·ª± ki·∫øn', 'd·ª± ƒëo√°n', 'd·ª± ph√≤ng',
            'd·ª± tr·ªØ', 'd·ª± th·∫ßu', 'd·ª± thi', 'tham d·ª±', 'd·ª± ƒë·ªãnh',
            
            # ===== D≈©ng/D∆∞·ª°ng family =====
            'd≈©ng c·∫£m', 'anh d≈©ng', 'd≈©ng sƒ©', 'd≈©ng m√£nh',
            'd∆∞·ª°ng', 'b·∫£o d∆∞·ª°ng', 'chƒÉm d∆∞·ª°ng', 'dinh d∆∞·ª°ng', 'tu d∆∞·ª°ng',
            
            # ===== Gi√°o d·ª•c context =====
            'gi√°o d·ª•c', 'ƒë√†o t·∫°o', 'hu·∫•n luy·ªán', 'ƒë·∫Øc l·ª±c',
            
            # ===== PROPER NAMES - NEED WHITELIST =====
            'ph√∫c du',  # Rapper Ph√∫c Du
            'du h√≠', 'du m·ª•c', 'du k√≠ch', 'du ƒë·∫£ng',
        ],
    },
    
    'dit': {
        'normalized': 'ƒë·ªãt',
        'variants': [
            'ƒë·ªãt', 'dit', 'ƒëit', 'd·ªãt', 'ƒë·ªãt',
            'ƒë.i.t', 'd.i.t', 'ƒë_i_t', 'd_i_t', 'ƒë-i-t', 'd-i-t',
            'ƒë i t', 'd i t', 'ƒë  i  t', 'd  i  t',
            'ƒë*t', 'd*t', 'ƒë!t', 'd!t', 'ƒë1t', 'd1t',
            'djt', 'ƒëjt', 'd√≠t', 'ƒë√≠t',
            # Leetspeak
            'd!7', 'ƒë!7', 'd17', 'ƒë17',
            # Unicode
            '…ó·ªãt', '…óit', 'ƒëƒ´t', 'dƒ´t',
        ],
        'severity': 'high',
    },
    
    # ===== L·ªíN family =====
    'lon': {
        'normalized': 'l·ªìn',
        'variants': [
            'l·ªìn', 'lon', 'l√≤n', 'l√≥n', 'l·ªën', 'l·ªïn', 'l·ªôn',
            'l.o.n', 'l_o_n', 'l-o-n', 'l o n', 'l  o  n',
            'l*n', 'l@n', 'l0n', 'l0.n', 'l.0.n',
            'l·ªìl', 'lol', 'lonn', 'lonnn',
            # Leetspeak
            '10n', '1on', 'l0n', '10≈ã',
            # Unicode
            '…≠·ªìn', '…≠on', '≈Ç·ªìn', '≈Çon',
        ],
        'severity': 'high',
        'safe_contexts': ['h√†i l√≤ng', 'vui l√≤ng', 'l√≤ng tin', 'lon bia', 'bia lon', 'lon n∆∞·ªõc'],
    },
    
    # ===== C·∫∂C family =====
    'cac': {
        'normalized': 'c·∫∑c',
        'variants': [
            'c·∫∑c', 'cac', 'cak', 'c·∫Øc', 'c·∫°c', 'cƒÉc',
            'c.a.c', 'c_a_c', 'c-a-c', 'c a c', 'c  a  c',
            'c*c', 'c@c', 'c4c', 'kac', 'k·∫∑c',
            'cacc', 'caccc',
            # Leetspeak  
            '(4(', 'c4c', '(a(',
            # Unicode
            'œÇ·∫∑c', 'œÇac', '√ß·∫∑c', '√ßac',
        ],
        'severity': 'high',
        'safe_contexts': ['c√°c b·∫°n', 'c√°c anh', 'c√°c ch·ªã', 'm·ªôt c√°ch', 'b·∫±ng c√°ch'],
    },
    
    # ===== VCL/VL family =====
    'vcl': {
        'normalized': 'vcl',
        'variants': [
            'vcl', 'vkl', 'v.c.l', 'v_c_l', 'v-c-l', 'v c l',
            'v√£i l·ªìn', 'vai lon', 'v√£i lon', 'vai l·ªìn',
            'v·ªù c·ªù l·ªù', 'vo co lo', 'v∆° c∆° l∆°',
            # Short forms
            'vl', 'v.l', 'v_l', 'v-l', 'v l',
            # Leetspeak
            'v(1', 'vc1', 'vk1', '\\/cl', '\\/l',
            # Unicode
            'ŒΩcl', 'ŒΩl', '—µcl', '—µl',
        ],
        'severity': 'high',
    },
    
    # ===== ƒêM/DCM family =====
    'dm': {
        'normalized': 'ƒëm',
        'variants': [
            'ƒëm', 'dm', 'ƒë.m', 'd.m', 'ƒë_m', 'd_m', 'ƒë-m', 'd-m',
            'ƒë m', 'd m', 'ƒë  m', 'd  m',
            'ƒëmm', 'dmm', 'ƒëmmm', 'dmmm',
            'ƒëcm', 'dcm', 'ƒë.c.m', 'd.c.m',
            'ƒë·ªù m·ªù', 'do mo', 'ƒë∆° m∆°', 'd∆° m∆°',
            # Full forms
            'ƒë·ª• m√°', 'du ma', 'ƒë·ªãt m·∫π', 'dit me', 'ƒë·ª• m·∫π', 'du me',
            # Leetspeak
            'ƒë/m', 'd/m', '|)m', 'ƒë|\/|',
            # Unicode
            '…óm', '…ó.m', 'ƒë…±',
        ],
        'severity': 'high',
    },
    
    # ===== CC family =====
    'cc': {
        'normalized': 'cc',
        'variants': [
            'cc', 'c.c', 'c_c', 'c-c', 'c c',
            'c·ªù c·ªù', 'co co', 'c∆° c∆°',
            # Leetspeak
            '((', 'c(', '(c',
            # Unicode
            'œÇœÇ', '√ß√ß',
        ],
        'severity': 'medium',
    },
    
    # ===== Ch·∫øt ti·ªát family =====
    'chettiet': {
        'normalized': 'ch·∫øt ti·ªát',
        'variants': [
            'ch·∫øt ti·ªát', 'chet tiet', 'ch·∫øt ti√™t', 'chet tiet',
            'ch.·∫ø.t', 'c.h.e.t', 'ch*t', 'ch3t',
        ],
        'severity': 'low',
    },
    
    # ===== Ngu family =====
    'ngu': {
        'normalized': 'ngu',
        'variants': [
            # ONLY match CLEAR obfuscation patterns - DO NOT match standalone 'ngu'
            'nguu', 'nguuu', 'nq∆∞',
            'n.g.u', 'n_g_u', 'n-g-u', 'n g u',
            'nqu',
            # Leetspeak
            'n9u', 'ngu7',
            # Unicode
            '≈ãgu', '≈ãu',
            # NOTE: 'ngu' alone REMOVED - too common as substring in:
            # ng∆∞·ªùi, nh·ªØng, nguy, ngu·ªìn, ng·ªß, nguy·ªÖn, nguy√™n, etc.
        ],
        'severity': 'medium',
        'context_dependent': True,  # VERY context dependent!
        # CRITICAL: All Vietnamese words containing 'ngu' substring
        'safe_contexts': [
            # Common words containing 'ngu' - NOT an insult
            'ngu·ªìn', 'ng·ªß', 'ng≈©',
            'nguy·ªÖn', 'nguy√™n', 'nguyen', 'nguy·ªÉn',
            'ng·ªßi', 'ng·ª©a', 'ng·ª±a', 'ng·ª•',
            'ngu·ªôi', 'ngu·ªìi', 'ng∆∞·ªõc', 'ngu·ªách', 'nguy·ªán',
            'ngu ng·ªëc',  # Explicit insult - still need combo context
            'c·∫©n ng∆∞',  # Fishing related
            'ng∆∞ d√¢n', 'ng∆∞ nghi·ªáp', 'ng∆∞ tr∆∞·ªùng', 'ng∆∞ l∆∞·ªõi',
            # Confucianism / Philosophy context
            'nho b√°c', 'kh·ªïng gi√°o', 'nho gi√°o',
            # Common verb patterns
            'm·ªçi ng∆∞·ªùi', 'nhi·ªÅu ng∆∞·ªùi', 'ai ng∆∞·ªùi', 'c√≤n ng∆∞·ªùi',
            'con ng∆∞·ªùi', 'c·ªßa ng∆∞·ªùi', 'cho ng∆∞·ªùi', 'v·ªõi ng∆∞·ªùi',
            't·ª´ ng∆∞·ªùi', 'ƒë·∫øn ng∆∞·ªùi', 'nh∆∞ ng∆∞·ªùi', 'l√† ng∆∞·ªùi',
            'c√≥ ng∆∞·ªùi', 'v√† ng∆∞·ªùi', 'ƒë∆∞·ª£c ng∆∞·ªùi', 'b·ªüi ng∆∞·ªùi',
        ],
    },
    
    # ===== ƒêi√™n/Kh√πng family =====
    'dien': {
        'normalized': 'ƒëi√™n',
        'variants': [
            'ƒëi√™n', 'dien', 'ƒëien', 'ƒëi·ªán', 'ƒë√¨√™n',
            'ƒë.i.√™.n', 'd.i.e.n',
            'ƒëi√™n kh√πng', 'dien khung',
        ],
        'severity': 'medium',
        'context_dependent': True,
    },
    
    # ===== Kh·ªën n·∫°n family =====
    'khonnan': {
        'normalized': 'kh·ªën n·∫°n',
        'variants': [
            'kh·ªën n·∫°n', 'khon nan', 'kh·ªën nan', 'khon n·∫°n',
            'kh·ªën ki·∫øp', 'khon kiep', 'kh·ªën ki√™p',
            'k.h.·ªë.n', 'kh*n', 'kh√¥n n·∫°n',
        ],
        'severity': 'medium',
    },
}

# ==================== ADVANCED PATTERN DETECTION ====================

# Regex patterns for complex obfuscation
ADVANCED_OBFUSCATION_PATTERNS = [
    # Zero-width characters
    (r'[\u200b\u200c\u200d\u2060\ufeff]', ''),
    
    # Combining diacritical marks abuse
    (r'[\u0300-\u036f]', ''),
    
    # Invisible characters
    (r'[\u2000-\u200a\u2028\u2029\u202f\u205f\u3000]', ' '),
    
    # Repeated spaces (bypass attempt)
    (r'\s{2,}', ' '),
    
    # Dots/dashes between every character
    (r'([a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë])[.\-_~\*]{1,2}(?=[a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë])', r'\1'),
    
    # Emoji spam between letters (common bypass)
    (r'([a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë])[\U0001F300-\U0001F9FF]{1,3}(?=[a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë])', r'\1'),
]


class VariantDetector:
    """
    Detects variants and obfuscated forms of toxic words
    """
    
    def __init__(self):
        self.toxic_variants = TOXIC_VARIANTS
        self.homoglyphs = HOMOGLYPHS
        self.leetspeak = LEETSPEAK_TO_LETTER
        self.obfuscation_patterns = [(re.compile(p), r) for p, r in ADVANCED_OBFUSCATION_PATTERNS]
        
        # Build variant lookup
        self._build_variant_index()
    
    def _build_variant_index(self):
        """Build reverse index from variant -> normalized form"""
        self.variant_to_normalized = {}
        self.variant_severity = {}
        
        for key, info in self.toxic_variants.items():
            normalized = info['normalized']
            severity = info.get('severity', 'medium')
            
            for variant in info['variants']:
                variant_lower = variant.lower()
                self.variant_to_normalized[variant_lower] = normalized
                self.variant_severity[variant_lower] = severity
    
    def normalize_homoglyphs(self, text: str) -> Tuple[str, List[str]]:
        """
        Replace homoglyph characters with their ASCII equivalents
        
        Returns:
            (normalized_text, list of replaced characters)
        """
        result = []
        replacements = []
        
        for char in text:
            if char in self.homoglyphs:
                replacement = self.homoglyphs[char]
                result.append(replacement)
                replacements.append(f"{char} -> {replacement}")
            else:
                result.append(char)
        
        return ''.join(result), replacements
    
    def normalize_leetspeak(self, text: str) -> Tuple[str, List[str]]:
        """
        Convert leetspeak characters to letters
        
        Returns:
            (normalized_text, list of conversions)
        """
        result = []
        conversions = []
        
        for char in text:
            if char in self.leetspeak:
                replacement = self.leetspeak[char]
                result.append(replacement)
                conversions.append(f"{char} -> {replacement}")
            else:
                result.append(char)
        
        return ''.join(result), conversions
    
    def remove_insertion_chars(self, text: str) -> Tuple[str, int]:
        """
        Remove inserted characters between letters
        
        Returns:
            (cleaned_text, count of removed chars)
        """
        count = 0
        result = text
        
        for char in INSERTION_CHARS:
            if char in result:
                # Only remove if between letters
                pattern = f'([a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]){re.escape(char)}([a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë])'
                new_result = re.sub(pattern, r'\1\2', result, flags=re.IGNORECASE)
                if new_result != result:
                    count += 1
                    result = new_result
        
        return result, count
    
    def apply_obfuscation_patterns(self, text: str) -> Tuple[str, int]:
        """
        Apply advanced obfuscation detection patterns
        
        Returns:
            (cleaned_text, count of patterns applied)
        """
        result = text
        count = 0
        
        for pattern, replacement in self.obfuscation_patterns:
            new_result = pattern.sub(replacement, result)
            if new_result != result:
                count += 1
                result = new_result
        
        return result, count
    
    def normalize_repeated_chars(self, text: str) -> str:
        """
        Normalize repeated characters (e.g., 'nguuuuu' -> 'nguu')
        """
        # Reduce 3+ repeated chars to 2
        return re.sub(r'(.)\1{2,}', r'\1\1', text)
    
    def full_normalize(self, text: str) -> Tuple[str, Dict[str, Any]]:
        """
        Apply all normalization steps
        
        Returns:
            (fully_normalized_text, metadata)
        """
        metadata = {
            'original': text,
            'homoglyphs': [],
            'leetspeak': [],
            'insertions': 0,
            'obfuscation_patterns': 0,
        }
        
        # Step 1: Homoglyphs
        text, homoglyphs = self.normalize_homoglyphs(text)
        metadata['homoglyphs'] = homoglyphs
        
        # Step 2: Leetspeak
        text, leetspeak = self.normalize_leetspeak(text)
        metadata['leetspeak'] = leetspeak
        
        # Step 3: Insertion chars
        text, insertions = self.remove_insertion_chars(text)
        metadata['insertions'] = insertions
        
        # Step 4: Advanced patterns
        text, patterns = self.apply_obfuscation_patterns(text)
        metadata['obfuscation_patterns'] = patterns
        
        # Step 5: Repeated chars
        text = self.normalize_repeated_chars(text)
        
        # Step 6: Lowercase for matching
        text = text.lower()
        
        metadata['normalized'] = text
        metadata['has_obfuscation'] = (
            len(homoglyphs) > 0 or
            len(leetspeak) > 0 or
            insertions > 0 or
            patterns > 0
        )
        
        return text, metadata
    
    def detect_variants(self, text: str) -> List[Dict[str, Any]]:
        """
        Detect toxic word variants in text
        
        Returns:
            List of detected variants with info
        """
        # First normalize
        normalized, metadata = self.full_normalize(text)
        
        detected = []
        
        # Check each word
        words = normalized.split()
        for word in words:
            # Clean word for matching
            clean_word = re.sub(r'[^\w]', '', word)
            
            if clean_word in self.variant_to_normalized:
                detected.append({
                    'original': word,
                    'normalized': self.variant_to_normalized[clean_word],
                    'severity': self.variant_severity.get(clean_word, 'medium'),
                    'type': 'exact_variant'
                })
                continue
            
            # Check if word contains a variant as substring
            # BUT only if it's NOT part of a larger legitimate word
            for variant, normalized_form in self.variant_to_normalized.items():
                if len(variant) >= 2 and variant in clean_word:
                    # Skip if the word is longer than the variant by more than 2 chars
                    # (likely a legitimate word containing the variant as substring)
                    if len(clean_word) > len(variant) + 2:
                        continue
                    
                    # Skip common Vietnamese words that contain toxic substrings
                    vietnamese_safe_words = {
                        'ng∆∞·ªùi', 'nh·ªØng', 'nguy', 'ngu·ªìn', 'ng·ªß', 'ng≈©',
                        'nguy·ªÖn', 'nguy√™n', 'nguyen', 'nguy·ªán', 'ngu·ªôi',
                        'ng∆∞·ªõc', 'ng·ª±a', 'ng·ª©a', 'ng·ª•', 'ng≈©i', 'ngu·ªách',
                        'd·ª•ng', 'd·ª•', 'd∆∞·ª°ng', 'd≈©ng', 'dung', 'd·ª±', 'd·ªØ', 'd∆∞',
                        'gi√°o', 'hu·∫•n', 't·∫≠p', 'luy·ªán', 'm·ª•c', 'ƒë√≠ch',
                        'ph·ª•c', 't√πng', 'ƒë√†y', 't·ªõ', 'ƒë·∫Øc', 'l·ª±c',
                        'ch∆∞∆°ng', 'tr√¨nh', 'tr·ªçng', 'tr√°ch', 'ti·∫øt', 'th√°o',
                        'uy√™n', 'th√¢m', 'ngh·ªãch', 'c·∫£nh', 'thu·∫≠n',
                        'tr√≠', 'hu·ªá',  # Philosophy terms
                    }
                    if clean_word in vietnamese_safe_words:
                        continue
                    
                    detected.append({
                        'original': word,
                        'variant_found': variant,
                        'normalized': normalized_form,
                        'severity': self.variant_severity.get(variant, 'medium'),
                        'type': 'substring_variant'
                    })
                    break
        
        # Also check for multi-word variants
        for key, info in self.toxic_variants.items():
            for variant in info['variants']:
                if ' ' in variant and variant in normalized:
                    detected.append({
                        'original': variant,
                        'normalized': info['normalized'],
                        'severity': info.get('severity', 'medium'),
                        'type': 'phrase_variant'
                    })
        
        return detected
    
    def is_safe_context(self, text: str, normalized_word: str) -> bool:
        """
        Check if the word appears in a safe context
        """
        text_lower = text.lower()
        
        # Find the variant info
        for key, info in self.toxic_variants.items():
            if info['normalized'] == normalized_word:
                safe_contexts = info.get('safe_contexts', [])
                for context in safe_contexts:
                    if context in text_lower:
                        return True
        
        return False
    
    def analyze(self, text: str) -> Dict[str, Any]:
        """
        Full analysis of text for variants
        
        Returns:
            Analysis result dict
        """
        normalized, metadata = self.full_normalize(text)
        variants = self.detect_variants(text)
        
        # Filter out safe context variants
        actual_violations = []
        safe_context_matches = []
        
        for variant in variants:
            if self.is_safe_context(text, variant['normalized']):
                safe_context_matches.append(variant)
            else:
                actual_violations.append(variant)
        
        # Calculate severity
        if any(v['severity'] == 'high' for v in actual_violations):
            overall_severity = 'high'
        elif any(v['severity'] == 'medium' for v in actual_violations):
            overall_severity = 'medium'
        elif actual_violations:
            overall_severity = 'low'
        else:
            overall_severity = 'none'
        
        return {
            'original_text': text,
            'normalized_text': normalized,
            'normalization_metadata': metadata,
            'detected_variants': actual_violations,
            'safe_context_matches': safe_context_matches,
            'overall_severity': overall_severity,
            'has_violations': len(actual_violations) > 0,
            'has_obfuscation': metadata['has_obfuscation'],
        }


# Singleton instance
_detector_instance = None

def get_variant_detector() -> VariantDetector:
    """Get singleton instance"""
    global _detector_instance
    if _detector_instance is None:
        _detector_instance = VariantDetector()
    return _detector_instance


if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    
    detector = get_variant_detector()
    
    test_cases = [
        # Normal text
        "S·∫£n ph·∫©m t·ªët qu√°",
        
        # Standard toxic
        "ƒê·ªì ngu v√£i l·ªìn",
        
        # Leetspeak variants
        "ƒê0 ngu v@i l0n",
        "ƒë!t m·∫π m√†y",
        "d.m m√†y ngu v.l",
        
        # Homoglyph variants
        "ŒΩcl ƒë·ªì ngœÖ",  # Using Greek letters
        
        # Insertion variants
        "ƒë.·ª• m.√°",
        "l-o-n m√†y",
        "v*c*l*",
        
        # Safe context
        "H√†i l√≤ng v·ªõi d·ªãch v·ª•",
        "C√°c b·∫°n c√≥ kh·ªèe kh√¥ng?",
        
        # Complex obfuscation
        "üî•ƒëüî•·ª•üî•müî•√°üî•",  # Emoji insertion
        "ƒë‚Äã·ª•‚Äãm‚Äã√°",  # Zero-width spaces
    ]
    
    print("=" * 80)
    print("VARIANT DETECTION TEST")
    print("=" * 80)
    
    for text in test_cases:
        print(f"\nüìù Text: '{text}'")
        result = detector.analyze(text)
        
        print(f"   Normalized: '{result['normalized_text']}'")
        print(f"   Has Obfuscation: {result['has_obfuscation']}")
        print(f"   Severity: {result['overall_severity']}")
        
        if result['detected_variants']:
            print(f"   ‚ö†Ô∏è Detected: {[v['normalized'] for v in result['detected_variants']]}")
        if result['safe_context_matches']:
            print(f"   ‚úÖ Safe context: {[v['normalized'] for v in result['safe_context_matches']]}")
        
        if result['normalization_metadata']['homoglyphs']:
            print(f"   üî§ Homoglyphs: {result['normalization_metadata']['homoglyphs']}")
        if result['normalization_metadata']['leetspeak']:
            print(f"   üî¢ Leetspeak: {result['normalization_metadata']['leetspeak']}")
        
        print("-" * 60)
