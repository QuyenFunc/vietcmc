"""
Comprehensive Test Suite for Vietnamese Content Moderation
==========================================================

Tests all 3 layers:
- Layer A: Text normalization & anti-obfuscation
- Layer B: Enhanced rule-based check
- Layer C: ML model (integration test if available)

Categories tested:
- Profanity (standard + obfuscated)
- Hate speech (racism, xenophobia, LGBTQ+)
- Harassment / Body-shaming
- Sexual content
- Safe content (false positive prevention)

Version: 1.0.0
Last Updated: 2026-01-30
"""

import logging
import sys
import os
from typing import Dict, List, Tuple, Any
from datetime import datetime

# Setup path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import modules
try:
    from nlp.text_normalizer import get_normalizer
    from nlp.rule_checker import get_rule_checker
    from nlp.moderation_pipeline import ThreeLayerModerationPipeline
    MODULES_AVAILABLE = True
except ImportError as e:
    print(f"Import error: {e}")
    MODULES_AVAILABLE = False


# ==================== TEST CASES ====================

# Format: (input_text, expected_action, expected_labels, description)
# expected_action: 'reject', 'review', 'allowed'
# expected_labels: list of labels that should be present (can be empty for 'allowed')

TEST_CASES = {
    # ===== PROFANITY: Standard =====
    'profanity_standard': [
        ("ƒëm m√†y", 'reject', ['profanity', 'toxicity'], "Direct profanity - ƒëm"),
        ("vcl", 'reject', ['profanity', 'toxicity'], "Abbreviation - vcl"),
        ("dm con ch√≥", 'reject', ['profanity', 'toxicity'], "DM with context"),
        ("ƒëcm m√†y ngu", 'reject', ['profanity', 'toxicity'], "DCM variant"),
        ("ƒë·ªãt m·∫π m√†y", 'reject', ['profanity', 'toxicity'], "Full phrase"),
        ("ƒë·ª• m√°", 'reject', ['profanity', 'toxicity'], "ƒê·ª• m√° variant"),
        ("vl lu√¥n", 'reject', ['profanity', 'toxicity'], "VL abbreviation"),
        ("clm", 'reject', ['profanity', 'toxicity'], "CLM abbreviation"),
        ("ctm", 'reject', ['profanity', 'toxicity'], "CTM abbreviation"),
    ],
    
    # ===== PROFANITY: Obfuscated =====
    'profanity_obfuscated': [
        ("d.m", 'reject', ['profanity', 'toxicity'], "Dot separator"),
        ("ƒë.m", 'reject', ['profanity', 'toxicity'], "Dot separator with ƒë"),
        ("d:m", 'reject', ['profanity', 'toxicity'], "Colon separator"),
        ("d:m,m", 'reject', ['profanity', 'toxicity'], "Multiple separators"),
        ("d*m", 'reject', ['profanity', 'toxicity'], "Asterisk separator"),
        ("d-m", 'reject', ['profanity', 'toxicity'], "Dash separator"),
        ("d_m", 'reject', ['profanity', 'toxicity'], "Underscore separator"),
        ("d  m", 'reject', ['profanity', 'toxicity'], "Double space"),
        ("ƒë.·ªã.t", 'reject', ['profanity', 'toxicity'], "Full obfuscation"),
        ("l.o.n", 'reject', ['profanity', 'toxicity'], "LON obfuscated"),
        ("c.a.c", 'reject', ['profanity', 'toxicity'], "CAC obfuscated - should NOT match 'c√°c'"),
        ("v.c.l", 'reject', ['profanity', 'toxicity'], "VCL obfuscated"),
        ("d–º–º", 'reject', ['profanity', 'toxicity'], "Cyrillic –º"),
        ("ŒΩcl", 'reject', ['profanity', 'toxicity'], "Greek ŒΩ"),
        ("ÔΩÑÔΩç", 'reject', ['profanity', 'toxicity'], "Full-width chars"),
        ("ƒë‚Äãm‚Äãm", 'reject', ['profanity', 'toxicity'], "Zero-width spaces"),
        ("d1t me", 'reject', ['profanity', 'toxicity'], "Leetspeak 1=i"),
        ("l0n", 'reject', ['profanity', 'toxicity'], "Leetspeak 0=o"),
        ("c@c", 'reject', ['profanity', 'toxicity'], "Leetspeak @=a"),
    ],
    
    # ===== PROFANITY: Context-dependent (NGU) =====
    'profanity_ngu': [
        ("ngu qu√°", 'review', ['insult'], "Ngu with intensifier"),
        ("ngu ng·ªëc", 'review', ['insult'], "Ngu ng·ªëc compound"),
        ("th·∫±ng n√†y ngu", 'review', ['insult'], "Targeting with ngu"),
        ("ƒë·ªì ngu ng·ªëc", 'review', ['insult'], "ƒê·ªì ngu ng·ªëc pattern"),
        ("n.g.u", 'review', ['insult', 'obfuscation_bypass'], "Obfuscated ngu"),
        ("n√£o c√° v√†ng", 'review', ['insult'], "Brain insult"),
        ("ƒë·∫ßu ƒë·∫•t", 'review', ['insult'], "Head insult"),
    ],
    
    # ===== HATE SPEECH: Racism =====
    'hate_speech_racism': [
        ("B·ªçn da ƒëen b·∫©n th·ªâu c√∫t v·ªÅ n∆∞·ªõc ƒëi", 'reject', ['hate', 'racism'], "Anti-black racism"),
        ("b·ªçn da ƒëen", 'reject', ['hate', 'racism'], "Group targeting blacks"),
        ("l≈© m·ªçi ƒëen", 'reject', ['hate', 'racism'], "Derogatory term for blacks"),
        ("kh·ªâ ƒëen", 'reject', ['hate', 'racism'], "Animal dehumanization"),
        ("t√†u kh·ª±a", 'reject', ['hate', 'racism'], "Anti-Chinese slur"),
        ("b·ªçn t√†u", 'reject', ['hate', 'racism'], "Group targeting Chinese"),
        ("l≈© t√†u c·ªông", 'reject', ['hate', 'racism'], "Political anti-Chinese"),
    ],
    
    # ===== HATE SPEECH: LGBTQ+ =====
    'hate_speech_lgbtq': [
        ("ƒë·ªì gay", 'reject', ['hate', 'lgbtq_discrimination'], "Anti-gay insult"),
        ("b·ªçn p√™ ƒë√™", 'reject', ['hate', 'lgbtq_discrimination'], "Anti-gay slur"),
        ("th·∫±ng b√™ ƒë√™", 'reject', ['hate', 'lgbtq_discrimination'], "Individual targeting"),
        ("gay ƒë√°ng ch·∫øt", 'reject', ['hate', 'lgbtq_discrimination'], "Death wish"),
    ],
    
    # ===== HATE SPEECH: Xenophobia =====
    'hate_speech_xenophobia': [
        ("c√∫t v·ªÅ n∆∞·ªõc ƒëi b·ªçn da ƒëen", 'reject', ['hate', 'xenophobia'], "Xenophobic + racial"),
        ("bi·∫øn ƒëi ng∆∞·ªùi n∆∞·ªõc ngo√†i", 'reject', ['hate', 'xenophobia'], "Anti-foreigner with context"),
    ],
    
    # ===== HARASSMENT: Body-shaming =====
    'harassment_bodyshaming': [
        ("Sao m√†y x·∫•u th·∫ø, nh√¨n m·∫∑t m√†y tao mu·ªën n√¥n", 'reject', ['harassment', 'body_shaming'], "Severe body-shaming - escalated"),
        ("m√†y x·∫•u qu√°", 'review', ['harassment', 'body_shaming'], "Simple body-shaming"),
        ("m·∫∑t m√†y gh√™ t·ªüm", 'reject', ['harassment', 'body_shaming'], "Severe expression - escalated"),
        ("n√≥ x·∫•u x√≠ kinh t·ªüm", 'reject', ['harassment', 'body_shaming'], "Multiple severe expressions"),
        ("m√†y x·∫•u", 'review', ['harassment', 'body_shaming'], "Basic body-shaming"),
        ("mi b·∫©n th·ªâu", 'review', ['harassment', 'body_shaming'], "Cleanliness attack"),
    ],
    
    # ===== HARASSMENT: Personal attacks =====
    'harassment_personal': [
        ("ƒë·ªì ngu", 'review', ['harassment', 'insult'], "ƒê·ªì ngu pattern"),
        ("th·∫±ng kh·ªën", 'review', ['harassment', 'insult'], "Th·∫±ng + insult"),
        ("con ƒëi√™n", 'review', ['harassment', 'insult'], "Con + insult"),
        ("ƒë·ªì v√¥ d·ª•ng", 'review', ['harassment', 'insult'], "ƒê·ªì v√¥ d·ª•ng"),
        ("ƒë·ªì s√∫c v·∫≠t", 'review', ['harassment', 'insult'], "Dehumanizing"),
    ],
    
    # ===== SAFE CONTENT: Valid feedback =====
    'safe_feedback': [
        ("S·∫£n ph·∫©m t·ªët qu√°", 'allowed', [], "Positive feedback"),
        ("T√¥i kh√¥ng h√†i l√≤ng v·ªõi d·ªãch v·ª•", 'allowed', [], "Valid complaint"),
        ("S·∫£n ph·∫©m t·ªá qu√°, th·∫•t v·ªçng", 'allowed', [], "Negative but valid feedback"),
        ("Ch·∫•t l∆∞·ª£ng kh√¥ng nh∆∞ mong ƒë·ª£i", 'allowed', [], "Disappointed but valid"),
        ("Giao h√†ng ch·∫≠m, c·∫ßn c·∫£i thi·ªán", 'allowed', [], "Constructive criticism"),
        ("M√¨nh kh√¥ng recommend s·∫£n ph·∫©m n√†y", 'allowed', [], "Honest review"),
        ("T·ªá qu√°, 1 sao", 'allowed', [], "Low rating but valid"),
    ],
    
    # ===== SAFE CONTENT: False positive prevention =====
    'safe_false_positives': [
        ("Lon bia n√†y ngon", 'allowed', [], "Lon bia context"),
        ("N∆∞·ªõc lon pepsi", 'allowed', [], "Lon n∆∞·ªõc context"),
        ("C√°c b·∫°n c√≥ kh·ªèe kh√¥ng?", 'allowed', [], "C√°c b·∫°n - not c·∫∑c"),
        ("H√†i l√≤ng v·ªõi d·ªãch v·ª•", 'allowed', [], "L√≤ng - not l·ªìn"),
        ("C√°c lo·∫°i s·∫£n ph·∫©m", 'allowed', [], "C√°c lo·∫°i - not c·∫∑c"),
        ("M·ªôt c√°ch ti·ªán l·ª£i", 'allowed', [], "C√°ch - not c·∫∑c"),
        ("Du l·ªãch ƒê√† N·∫µng", 'allowed', [], "Du l·ªãch - not ƒë·ª•"),
        ("Du h·ªçc sinh", 'allowed', [], "Du h·ªçc - not ƒë·ª•"),
        ("Nguy·ªÖn vƒÉn A", 'allowed', [], "Ngu·ªìn - not ngu"),
        ("Ngu·ªìn g·ªëc s·∫£n ph·∫©m", 'allowed', [], "Ngu·ªìn - not ngu"),
        ("Ng∆∞·ªùi b√°n h√†ng t·ªët", 'allowed', [], "Ng∆∞·ªùi - not ngu"),
        ("T·∫•m l√≤ng nh√¢n √°i", 'allowed', [], "L√≤ng - not l·ªìn"),
    ],
    
    # ===== EDGE CASES =====
    'edge_cases': [
        ("", 'allowed', [], "Empty string"),
        ("   ", 'allowed', [], "Whitespace only"),
        ("üëçüëçüëç", 'allowed', [], "Emoji only"),
        ("1234567890", 'allowed', [], "Numbers only"),
        ("abc", 'allowed', [], "Short text"),
        ("OK", 'allowed', [], "Very short text"),
        # Mixed content
        ("S·∫£n ph·∫©m dm t·ªët", 'reject', ['profanity', 'toxicity'], "DM hidden in sentence"),
        ("dm s·∫£n ph·∫©m t·ªët", 'reject', ['profanity', 'toxicity'], "DM at start"),
        ("s·∫£n ph·∫©m t·ªët dm", 'reject', ['profanity', 'toxicity'], "DM at end"),
    ],
}


# ==================== TEST RUNNER ====================

class TestRunner:
    """Run comprehensive tests and generate report"""
    
    def __init__(self):
        self.normalizer = get_normalizer()
        self.rule_checker = get_rule_checker()
        self.pipeline = ThreeLayerModerationPipeline(
            text_model=None,  # Rule-based only
            use_rule_based=True,
            use_ml_model=False,
        )
        
        self.results = {
            'total': 0,
            'passed': 0,
            'failed': 0,
            'categories': {},
        }
        self.failures = []
    
    def run_single_test(self, text: str, expected_action: str, expected_labels: List[str], description: str) -> bool:
        """Run a single test case"""
        result = self.pipeline.predict(text)
        
        actual_action = result.get('action', 'unknown')
        actual_labels = set(result.get('labels', []))
        expected_labels_set = set(expected_labels)
        
        # Check action
        action_match = actual_action == expected_action
        
        # Check labels (expected should be subset of actual for reject/review)
        if expected_action in ['reject', 'review']:
            labels_match = expected_labels_set.issubset(actual_labels)
        else:
            labels_match = len(actual_labels) == 0 or expected_labels_set == actual_labels
        
        passed = action_match and labels_match
        
        if not passed:
            self.failures.append({
                'text': text,
                'description': description,
                'expected_action': expected_action,
                'actual_action': actual_action,
                'expected_labels': list(expected_labels_set),
                'actual_labels': list(actual_labels),
                'reasoning': result.get('reasoning', ''),
            })
        
        return passed
    
    def run_category(self, category: str, test_cases: List[Tuple]) -> Dict:
        """Run all tests in a category"""
        category_results = {
            'total': len(test_cases),
            'passed': 0,
            'failed': 0,
        }
        
        for text, expected_action, expected_labels, description in test_cases:
            self.results['total'] += 1
            
            if self.run_single_test(text, expected_action, expected_labels, description):
                self.results['passed'] += 1
                category_results['passed'] += 1
            else:
                self.results['failed'] += 1
                category_results['failed'] += 1
        
        self.results['categories'][category] = category_results
        return category_results
    
    def run_all(self) -> Dict:
        """Run all test categories"""
        print("=" * 80)
        print("COMPREHENSIVE CONTENT MODERATION TEST SUITE")
        print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
        
        for category, test_cases in TEST_CASES.items():
            print(f"\nüìã Testing: {category}")
            results = self.run_category(category, test_cases)
            
            status = "‚úÖ" if results['failed'] == 0 else "‚ùå"
            print(f"   {status} {results['passed']}/{results['total']} passed")
        
        return self.results
    
    def print_summary(self):
        """Print test summary"""
        print("\n" + "=" * 80)
        print("TEST SUMMARY")
        print("=" * 80)
        
        total = self.results['total']
        passed = self.results['passed']
        failed = self.results['failed']
        
        pass_rate = (passed / total * 100) if total > 0 else 0
        
        print(f"\nüìä Overall: {passed}/{total} tests passed ({pass_rate:.1f}%)")
        
        print("\nüìÅ By Category:")
        for category, cat_results in self.results['categories'].items():
            status = "‚úÖ" if cat_results['failed'] == 0 else "‚ùå"
            cat_rate = (cat_results['passed'] / cat_results['total'] * 100) if cat_results['total'] > 0 else 0
            print(f"   {status} {category}: {cat_results['passed']}/{cat_results['total']} ({cat_rate:.1f}%)")
        
        if self.failures:
            print("\n‚ùå FAILURES:")
            print("-" * 60)
            for i, failure in enumerate(self.failures[:10], 1):  # Show first 10
                print(f"\n{i}. {failure['description']}")
                print(f"   Input: '{failure['text']}'")
                print(f"   Expected: {failure['expected_action']} with {failure['expected_labels']}")
                print(f"   Actual: {failure['actual_action']} with {failure['actual_labels']}")
                if failure['reasoning']:
                    print(f"   Reason: {failure['reasoning'][:60]}...")
            
            if len(self.failures) > 10:
                print(f"\n   ... and {len(self.failures) - 10} more failures")
        
        print("\n" + "=" * 80)
        
        return pass_rate >= 90  # Return True if pass rate >= 90%


# ==================== MAIN ====================

def main():
    """Main entry point"""
    if not MODULES_AVAILABLE:
        print("‚ùå Required modules not available. Exiting.")
        return False
    
    logging.basicConfig(level=logging.WARNING)  # Reduce noise
    
    runner = TestRunner()
    runner.run_all()
    success = runner.print_summary()
    
    if success:
        print("\nüéâ TEST SUITE PASSED!")
    else:
        print("\n‚ö†Ô∏è TEST SUITE NEEDS ATTENTION")
    
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
