"""
Advanced Context Analyzer for Vietnamese Content Moderation
- Context-aware classification: distinguish valid negative context vs toxic
- Semantic similarity checking: detect semantic variants
- Intent detection: determine user intent
- Multi-factor scoring: multi-factor evaluation

Version: 1.0.0
Last Updated: 2025-12-19
"""

import re
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class ContentIntent(Enum):
    """Classify content intent"""
    FEEDBACK_NEGATIVE = "feedback_negative"  # Negative review of product/service
    FEEDBACK_POSITIVE = "feedback_positive"  # Positive review
    COMPLAINT = "complaint"                  # Complaint/claim
    PERSONAL_ATTACK = "personal_attack"      # Personal attack
    HATE_SPEECH = "hate_speech"              # Hate speech
    SPAM = "spam"                            # Spam/advertising
    NEUTRAL = "neutral"                      # Neutral
    QUESTION = "question"                    # Question


@dataclass
class ContextAnalysisResult:
    """Context analysis result"""
    intent: ContentIntent
    confidence: float
    is_legitimate_criticism: bool
    targets_product: bool
    targets_person: bool
    has_valid_reason: bool
    severity_modifier: float  # 0.0 - 2.0 (< 1.0 = gi·∫£m, > 1.0 = tƒÉng)
    reasoning: str


# ==================== CONTEXT INDICATORS ====================

# Words indicating product/service reviews (LEGITIMATE FEEDBACK)
PRODUCT_REVIEW_INDICATORS = [
    # Product quality
    's·∫£n ph·∫©m', 'h√†ng', 'ƒë·ªì', 'm√≥n', 's·∫£n ph·∫©m n√†y', 'm√≥n n√†y', 'ƒë·ªì n√†y',
    'h√†ng n√†y', 'order', 'ƒë∆°n h√†ng', 'g√≥i h√†ng',
    
    # Service
    'd·ªãch v·ª•', 'ph·ª•c v·ª•', 'nh√¢n vi√™n', 'shop', 'c·ª≠a h√†ng', 'store',
    'giao h√†ng', 'ship', 'ƒë√≥ng g√≥i', 'bao b√¨', 'packaging',
    
    # Price
    'gi√°', 'ti·ªÅn', 'chi ph√≠', 'ph√≠', 'gi√° ti·ªÅn', 'ƒë·∫Øt', 'r·∫ª', 'ƒë√°ng ti·ªÅn',
    'kh√¥ng ƒë√°ng', 'h·ªùi', 'l·ªó', 'ƒë∆∞·ª£c gi√°', 'gi√° cao', 'gi√° th·∫•p',
    
    # Quality
    'ch·∫•t l∆∞·ª£ng', 'ch·∫•t li·ªáu', 'v·∫£i', 'm√†u', 'size', 'k√≠ch th∆∞·ªõc',
    'form', 'd√°ng', 'ki·ªÉu', 'm·∫´u', 'thi·∫øt k·∫ø', 'design',
    
    # Usage evaluation
    'd√πng', 's·ª≠ d·ª•ng', 'x√†i', 'm·∫∑c', 'ƒëi', 'ƒÉn', 'u·ªëng',
    'test', 'th·ª≠', 'review', 'ƒë√°nh gi√°',
    
    # Comparison
    'kh√°c h√¨nh', 'kh√¥ng gi·ªëng', 'gi·ªëng h√¨nh', 'nh∆∞ h√¨nh', 'ƒë√∫ng m√¥ t·∫£',
    'sai m√¥ t·∫£', 'kh√¥ng ƒë√∫ng', 'kh√°c xa',
]

# Words indicating VALID negative feedback about products
LEGITIMATE_NEGATIVE_FEEDBACK = [
    # Disappointment
    'th·∫•t v·ªçng', 'h·ª•t h·∫´ng', 'kh√¥ng h√†i l√≤ng', 'kh√¥ng v·ª´a √Ω', 'kh√¥ng ∆∞ng',
    'kh√¥ng ok', 'kh√¥ng ·ªïn', 'ch√°n', 't·ªá', 'd·ªü', 'k√©m',
    
    # Specific complaints
    'l·ªói', 'h·ªèng', 'r√°ch', 'b·ªÉ', 'v·ª°', 'kh√¥ng ho·∫°t ƒë·ªông', 'kh√¥ng ch·∫°y',
    'ch·∫≠m', 'tr·ªÖ', 'delay', 'giao mu·ªôn', 'ƒë·ª£i l√¢u', 'l√¢u qu√°',
    
    # Not worth the money
    'ph√≠ ti·ªÅn', 't·ªën ti·ªÅn', 'm·∫•t ti·ªÅn', 'l√£ng ph√≠', 'kh√¥ng ƒë√°ng',
    'ƒë·∫Øt v√¥ l√Ω', 'ƒë·∫Øt qu√°', 'ch·∫•t l∆∞·ª£ng k√©m', 'd·ªü', 't·ªá h·∫°i',
    
    # Recommendation
    'kh√¥ng n√™n mua', 'ƒë·ª´ng mua', 'tr√°nh xa', 'kh√¥ng recommend',
    'one star', '1 sao', 'ƒë√°nh gi√° th·∫•p',
    
    # Valid negative emotions
    'b·ª±c', 'kh·ªï', 'th·∫•t v·ªçng', 'bu·ªìn', 'ti·∫øc',
]

# Words indicating PERSONAL ATTACK (needs moderation)
PERSONAL_ATTACK_INDICATORS = [
    # Personal pronouns
    'm√†y', 'mi', 'm·∫ßy', 'tau', 'tao', 'th·∫±ng m√†y', 'con m√†y',
    'th·∫±ng kia', 'con kia', 'ƒë·ª©a kia', 'ng∆∞·ªùi kia',
    
    # Disrespectful terms
    'th·∫±ng', 'con', 'ƒë·ª©a', 'l≈©', 'b·ªçn', 't·ª•i', 'nh√≥m',
    'th·∫±ng ch·ªß shop', 'th·∫±ng shipper', 'con b√©', 'th·∫±ng cha',
    
    # Direct insults
    'ch·ª≠i m√†y', 'ƒë√°nh m√†y', 'gi·∫øt m√†y', 'ch√©m m√†y',
    'bi·∫øn ƒëi', 'c√∫t ƒëi', 'get lost',
]

# Words indicating HATE SPEECH (reject immediately)
HATE_SPEECH_GROUP_INDICATORS = [
    # LGBTQ+ groups
    'gay', 'les', 'ƒë·ªìng t√≠nh', 'p√™ ƒë√™', 'b√™ ƒë√™', 'chuy·ªÉn gi·ªõi',
    
    # Ethnic groups
    't√†u', 'kh·ª±a', 'm·ªçi', 'm∆∞·ªùng', 'th·ªï d√¢n', 'mi·ªÅn n√∫i',
    
    # Religious groups
    'ph·∫≠t t·ª≠', 'c√¥ng gi√°o', 'h·ªìi gi√°o', 't√≠n ƒë·ªì',
]

# Words showing contempt/hatred towards groups
HATRED_MODIFIERS = [
    'gh√©t', 'khinh', 'ch·∫øt ƒëi', 'n√™n ch·∫øt', 'bi·∫øn ƒëi', 'ti√™u di·ªát',
    'di·ªát', 'b·∫©n', 'd∆°', 't·ªüm', 'kinh t·ªüm', 'ƒë√°ng gh√©t', 'ƒë√°ng khinh',
    'b·ªánh ho·∫°n', 'bi·∫øn th√°i', 'ƒëi√™n', 'kh√πng', 'm·∫•t d·∫°y', 'v√¥ vƒÉn h√≥a',
]

# Spam keywords
SPAM_KEYWORDS = [
    'inbox', 'zalo', 'li√™n h·ªá', 'hotline', 'sdt', 's·ªë ƒëi·ªán tho·∫°i',
    'click', 'mua ngay', 'ƒë·∫∑t h√†ng ngay', 'khuy·∫øn m√£i', 'sale',
    'gi·∫£m gi√°', 'free', 'mi·ªÖn ph√≠', 'link', 'http', 'www',
    't·∫∑ng', 'nh·∫≠n ngay', 'c∆° h·ªôi', 'duy nh·∫•t', 'c√≥ h·∫°n',
]

# ==================== ALLOWED CONTEXT PATTERNS ====================

# Context reducing severity
SEVERITY_REDUCING_CONTEXTS = [
    # Negation
    (r'\b(?:kh√¥ng|ch·∫≥ng|ch·∫£|ƒë√¢u c√≥|ƒë√¢u|h√¥ng|ko|k)\b', 0.5),
    
    # Joking
    (r'\b(?:ƒë√πa th√¥i|ƒë√πa m√†|joke|kidding|vui th√¥i|n√≥i ƒë√πa|ƒë√πa)\b', 0.3),
    
    # Self-reference
    (r'\b(?:t√¥i|m√¨nh|em|t)\s+(?:ngu|d·ªët|k√©m)\b', 0.2),
    
    # Quote
    (r'["\'].*?["\']', 0.4),
    
    # Assumption
    (r'\b(?:n·∫øu|gi·∫£ s·ª≠|v√≠ d·ª•|imagine)\b', 0.5),
]

# Context increasing severity
SEVERITY_INCREASING_CONTEXTS = [
    # Threat
    (r'\b(?:tao|tau)\s+(?:s·∫Ω|s·∫Ω|ph·∫£i)\b', 1.5),
    
    # Repeated insults
    (r'(?:vcl|vl|ƒëm|dm|cc)\s*(?:vcl|vl|ƒëm|dm|cc)', 1.8),
    
    # Targeting a group
    (r'\b(?:b·ªçn|l≈©|t·ª•i|nh√≥m)\s+\w+', 1.5),
    
    # Call for violence
    (r'\b(?:gi·∫øt|ch√©m|ƒë√°nh|ƒë·∫≠p)\s+(?:h·∫øt|s·∫°ch|t·∫•t c·∫£)', 2.0),
]


# ==================== SAFE WORD PATTERNS ====================

# SAFE words/phrases despite containing toxic substrings
SAFE_WORD_PATTERNS = {
    # "gay" in positive context
    'gay': [
        r'\bh·ª©ng\s+gay\b',          # h·ª©ng gay = high interest
        r'\bvui\s+gay\b',           # vui gay = joyful
        r'\bgay\s+g·∫Øt\b',           # gay g·∫Øt = intense
        r'\bn√≥ng\s+gay\b',          # n√≥ng gay = burning hot
        r'\bgay\s+go\b',            # gay go = difficult
    ],
    
    # "cac" / "c√°c" in normal context
    'cac': [
        r'\bc√°c\s+(?:b·∫°n|anh|ch·ªã|em|√¥ng|b√†|s·∫£n ph·∫©m|d·ªãch v·ª•|lo·∫°i|nh√†)\b',
        r'\bc√°c\s+(?:b√°c|c√¥|ch√∫|th·∫ßy|c√¥|gi√°o)\b',
        r'\bm·ªôt\s+c√°ch\b',
        r'\bb·∫±ng\s+c√°ch\b',
        r'\btheo\s+c√°ch\b',
    ],
    
    # "lon" in normal context
    'lon': [
        r'\b(?:h√†i|vui|xin|l√†m ∆°n|l√†m)\s+l√≤ng\b',  # h√†i l√≤ng, vui l√≤ng
        r'\b(?:bia|n∆∞·ªõc|coca|pepsi|7up|lon\s+n∆∞·ªõc)\s+lon\b',  # lon bia
        r'\blon\s+(?:bia|n∆∞·ªõc|coca|pepsi)\b',
    ],
    
    # "dit" in other context
    'dit': [
        r'\bedit\b',                # edit
        r'\bcredit\b',              # credit
        r'\breadit\b',              # reddit
    ],
    
    # "du" in other context
    'du': [
        r'\bdu\s+l·ªãch\b',           # travel
        r'\bdu\s+h·ªçc\b',            # study abroad
        r'\bdu\s+kh√°ch\b',          # tourist
        r'\bdu\s+xu√¢n\b',           # spring travel
        r'\bh∆∞·ªõng\s+d·∫´n\s+du\b',    # tour guide
    ],
}


class ContextAnalyzer:
    """
    Advanced context analysis to determine intent and severity
    """
    
    def __init__(self):
        self.safe_patterns = self._compile_safe_patterns()
        self.reducing_patterns = [(re.compile(p, re.IGNORECASE), m) for p, m in SEVERITY_REDUCING_CONTEXTS]
        self.increasing_patterns = [(re.compile(p, re.IGNORECASE), m) for p, m in SEVERITY_INCREASING_CONTEXTS]
    
    def _compile_safe_patterns(self) -> Dict[str, List[re.Pattern]]:
        """Compile c√°c pattern an to√†n"""
        compiled = {}
        for word, patterns in SAFE_WORD_PATTERNS.items():
            compiled[word] = [re.compile(p, re.IGNORECASE) for p in patterns]
        return compiled
    
    def is_safe_context(self, text: str, flagged_word: str) -> bool:
        """
        Check if the flagged word is in a safe context
        
        Args:
            text: VƒÉn b·∫£n g·ªëc
            flagged_word: T·ª´ b·ªã flag l√† toxic
            
        Returns:
            True n·∫øu t·ª´ n·∫±m trong ng·ªØ c·∫£nh an to√†n
        """
        text_lower = text.lower()
        
        # Check safe patterns cho t·ª´ c·ª• th·ªÉ
        for word_key, patterns in self.safe_patterns.items():
            if word_key in flagged_word.lower():
                for pattern in patterns:
                    if pattern.search(text_lower):
                        logger.debug(f"Safe context detected for '{flagged_word}' in: {text[:50]}...")
                        return True
        
        return False
    
    def detect_intent(self, text: str) -> ContentIntent:
        """
        Determine content intent
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
            
        Returns:
            ContentIntent enum
        """
        text_lower = text.lower()
        
        # Check c√¢u h·ªèi
        if any(q in text_lower for q in ['?', 'l√†m sao', 'nh∆∞ th·∫ø n√†o', '·ªü ƒë√¢u', 'bao nhi√™u', 'khi n√†o', 'ai', 'c√°i g√¨']):
            return ContentIntent.QUESTION
        
        # Check spam
        spam_count = sum(1 for kw in SPAM_KEYWORDS if kw in text_lower)
        if spam_count >= 3:
            return ContentIntent.SPAM
        
        # Check hate speech
        has_group = any(grp in text_lower for grp in HATE_SPEECH_GROUP_INDICATORS)
        has_hatred = any(mod in text_lower for mod in HATRED_MODIFIERS)
        if has_group and has_hatred:
            return ContentIntent.HATE_SPEECH
        
        # Check personal attack
        personal_count = sum(1 for ind in PERSONAL_ATTACK_INDICATORS if ind in text_lower)
        if personal_count >= 2:
            return ContentIntent.PERSONAL_ATTACK
        
        # Check product review (negative or positive)
        product_count = sum(1 for ind in PRODUCT_REVIEW_INDICATORS if ind in text_lower)
        negative_count = sum(1 for fb in LEGITIMATE_NEGATIVE_FEEDBACK if fb in text_lower)
        
        if product_count >= 1:
            if negative_count >= 1:
                return ContentIntent.FEEDBACK_NEGATIVE
            # Check positive indicators
            positive_words = ['t·ªët', 'ƒë·∫πp', 'ok', '·ªïn', 'h√†i l√≤ng', 'th√≠ch', 'recommend', '∆∞ng', 'ch·∫•t l∆∞·ª£ng']
            if any(pw in text_lower for pw in positive_words):
                return ContentIntent.FEEDBACK_POSITIVE
            return ContentIntent.COMPLAINT
        
        return ContentIntent.NEUTRAL
    
    def analyze_target(self, text: str) -> Tuple[bool, bool]:
        """
        Analyze targeted objects
        
        Returns:
            (targets_product, targets_person)
        """
        text_lower = text.lower()
        
        # Check targets product
        targets_product = any(ind in text_lower for ind in PRODUCT_REVIEW_INDICATORS)
        
        # Check targets person
        targets_person = any(ind in text_lower for ind in PERSONAL_ATTACK_INDICATORS)
        
        return targets_product, targets_person
    
    def calculate_severity_modifier(self, text: str) -> float:
        """
        Calculate severity modifier
        
        Returns:
            float: 0.0 - 2.0 
            < 1.0 = gi·∫£m m·ª©c ƒë·ªô
            > 1.0 = tƒÉng m·ª©c ƒë·ªô
        """
        modifier = 1.0
        text_lower = text.lower()
        
        # Apply reducing patterns
        for pattern, reduce_factor in self.reducing_patterns:
            if pattern.search(text_lower):
                modifier *= reduce_factor
        
        # Apply increasing patterns
        for pattern, increase_factor in self.increasing_patterns:
            if pattern.search(text_lower):
                modifier *= increase_factor
        
        # Clamp to range
        return max(0.1, min(2.0, modifier))
    
    def analyze(self, text: str, flagged_words: List[str] = None) -> ContextAnalysisResult:
        """
        Comprehensive context analysis
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ph√¢n t√≠ch
            flagged_words: Danh s√°ch t·ª´ ƒë√£ b·ªã flag l√† toxic
            
        Returns:
            ContextAnalysisResult
        """
        # Detect intent
        intent = self.detect_intent(text)
        
        # Analyze targets
        targets_product, targets_person = self.analyze_target(text)
        
        # Calculate severity modifier
        severity_modifier = self.calculate_severity_modifier(text)
        
        # Check safe context for flagged words
        safe_word_count = 0
        if flagged_words:
            for word in flagged_words:
                if self.is_safe_context(text, word):
                    safe_word_count += 1
            
            # Adjust modifier if many flagged words are in safe context
            if safe_word_count > 0:
                safe_ratio = safe_word_count / len(flagged_words)
                severity_modifier *= (1 - safe_ratio * 0.5)
        
        # Determine if legitimate criticism
        is_legitimate = (
            intent in [ContentIntent.FEEDBACK_NEGATIVE, ContentIntent.COMPLAINT, ContentIntent.QUESTION]
            and targets_product
            and not targets_person
        )
        
        # Has valid reason?
        text_lower = text.lower()
        has_valid_reason = any(fb in text_lower for fb in LEGITIMATE_NEGATIVE_FEEDBACK)
        
        # Calculate confidence
        if intent == ContentIntent.HATE_SPEECH:
            confidence = 0.95
        elif intent == ContentIntent.PERSONAL_ATTACK:
            confidence = 0.85
        elif is_legitimate:
            confidence = 0.8
        else:
            confidence = 0.6
        
        # Build reasoning
        reasoning_parts = []
        reasoning_parts.append(f"Intent: {intent.value}")
        if targets_product:
            reasoning_parts.append("Mentions product/service")
        if targets_person:
            reasoning_parts.append("Targets individual")
        if is_legitimate:
            reasoning_parts.append("Valid feedback/criticism")
        if has_valid_reason:
            reasoning_parts.append("Specific reason provided")
        if safe_word_count > 0:
            reasoning_parts.append(f"{safe_word_count} words in safe context")
        
        return ContextAnalysisResult(
            intent=intent,
            confidence=confidence,
            is_legitimate_criticism=is_legitimate,
            targets_product=targets_product,
            targets_person=targets_person,
            has_valid_reason=has_valid_reason,
            severity_modifier=severity_modifier,
            reasoning=" | ".join(reasoning_parts)
        )


# ==================== SEMANTIC SIMILARITY CHECK ====================

class SemanticChecker:
    """
    Semantic similarity check to detect variants
    Uses simple techniques without external embeddings
    """
    
    # Synonyms of toxic words
    TOXIC_SYNONYMS = {
        # Ngu/d·ªët variations
        'ngu': ['ƒë·∫ßn', 'kh·ªù', 'ng·ªëc', 'd·ªët', 'u m√™', 'thi·ªÉu nƒÉng', 'ch·∫≠m hi·ªÉu', 'k√©m th√¥ng minh'],
        
        # X·∫•u/t·ªá variations
        't·ªá': ['d·ªü', 'ch√°n', 'k√©m', 'k√©m c·ªèi', 't·ªìi', 't·ªìi t·ªá', 'gh√™', 'kinh'],
        
        # L·ª´a ƒë·∫£o variations
        'l·ª´a': ['l·ª´a ƒë·∫£o', 'l·ª´a g·∫°t', 'gian l·∫≠n', 'd·ªëi tr√°', 'gian d·ªëi', 'b·ªãp b·ª£m', 'l·ªçc l·ª´a'],
        
        # ƒêe d·ªça variations
        'gi·∫øt': ['ch√©m', 'ƒë√¢m', 'b·∫Øn', 'ti√™u di·ªát', 'h·ªßy', 'x·ª≠', 'ra tay'],
    }
    
    # Alternative spellings of toxic words
    TOXIC_SPELLINGS = {
        'ngu': ['ngu', 'nguu', 'n.g.u', 'nqu', 'nq∆∞', 'ng∆∞', 'ng√π'],
        'dm': ['dm', 'ƒëm', 'd.m', 'ƒë.m', 'ƒë m', 'd m', 'ƒë·ªù m·ªù', 'do mo'],
        'vcl': ['vcl', 'vkl', 'v.c.l', 'v-c-l', 'v·ªù c·ªù l·ªù', 'v√£i l·ªìn'],
        'dit': ['dit', 'ƒë·ªãt', 'ƒë·ªãt', 'd!t', 'ƒë!t', 'd1t', 'ƒë1t'],
    }
    
    def __init__(self):
        # Build reverse map for quick lookup
        self.reverse_synonyms = {}
        for base_word, synonyms in self.TOXIC_SYNONYMS.items():
            for syn in synonyms:
                self.reverse_synonyms[syn.lower()] = base_word
        
        self.reverse_spellings = {}
        for base_word, spellings in self.TOXIC_SPELLINGS.items():
            for sp in spellings:
                self.reverse_spellings[sp.lower()] = base_word
    
    def normalize_spelling(self, text: str) -> Tuple[str, List[str]]:
        """
        Normalize spelling and return normalized words
        
        Returns:
            (normalized_text, list of detected variations)
        """
        text_lower = text.lower()
        detected = []
        
        # Check spellings
        words = text_lower.split()
        normalized_words = []
        
        for word in words:
            # Remove special chars for matching
            clean_word = re.sub(r'[^a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', '', word)
            
            if clean_word in self.reverse_spellings:
                base = self.reverse_spellings[clean_word]
                detected.append(f"{word} -> {base}")
                normalized_words.append(base)
            else:
                normalized_words.append(word)
        
        return ' '.join(normalized_words), detected
    
    def find_synonyms(self, text: str) -> List[str]:
        """
        Find synonyms of toxic words
        
        Returns:
            List of detected toxic synonyms
        """
        text_lower = text.lower()
        detected = []
        
        for word in text_lower.split():
            clean_word = re.sub(r'[^a-z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', '', word)
            if clean_word in self.reverse_synonyms:
                base = self.reverse_synonyms[clean_word]
                detected.append(f"{word} (~{base})")
        
        return detected
    
    def check(self, text: str) -> Dict[str, Any]:
        """
        Comprehensive semantic check
        
        Returns:
            Dict v·ªõi th√¥ng tin v·ªÅ c√°c bi·∫øn th·ªÉ ƒë∆∞·ª£c ph√°t hi·ªán
        """
        normalized, spelling_variations = self.normalize_spelling(text)
        synonyms = self.find_synonyms(text)
        
        return {
            'normalized_text': normalized,
            'spelling_variations': spelling_variations,
            'detected_synonyms': synonyms,
            'has_variations': len(spelling_variations) > 0 or len(synonyms) > 0
        }


# ==================== CONFIDENCE CALIBRATOR ====================

class ConfidenceCalibrator:
    """
    Adjust confidence score based on multiple factors
    """
    
    # Factors affecting confidence
    CONFIDENCE_FACTORS = {
        # Increase confidence
        'has_multiple_toxic_words': 0.15,
        'has_personal_attack': 0.20,
        'has_hate_group_target': 0.25,
        'short_toxic_only': 0.10,  # Short message with only toxic words
        
        # Decrease confidence  
        'is_product_review': -0.20,
        'has_valid_reason': -0.15,
        'is_question': -0.10,
        'long_context': -0.10,  # Long message with clear context
        'has_safe_context': -0.25,
    }
    
    def calibrate(
        self,
        base_confidence: float,
        text: str,
        context_result: ContextAnalysisResult,
        toxic_word_count: int = 0
    ) -> float:
        """
        Adjust confidence score
        
        Args:
            base_confidence: Confidence ban ƒë·∫ßu t·ª´ model
            text: VƒÉn b·∫£n g·ªëc
            context_result: K·∫øt qu·∫£ ph√¢n t√≠ch context
            toxic_word_count: S·ªë l∆∞·ª£ng t·ª´ toxic ƒë∆∞·ª£c ph√°t hi·ªán
            
        Returns:
            float: Confidence ƒë√£ ƒëi·ªÅu ch·ªânh (0.0 - 1.0)
        """
        adjustment = 0.0
        
        # TƒÉng confidence
        if toxic_word_count >= 2:
            adjustment += self.CONFIDENCE_FACTORS['has_multiple_toxic_words']
        
        if context_result.targets_person:
            adjustment += self.CONFIDENCE_FACTORS['has_personal_attack']
        
        if context_result.intent == ContentIntent.HATE_SPEECH:
            adjustment += self.CONFIDENCE_FACTORS['has_hate_group_target']
        
        if len(text.split()) <= 5 and toxic_word_count > 0:
            adjustment += self.CONFIDENCE_FACTORS['short_toxic_only']
        
        # Gi·∫£m confidence
        if context_result.targets_product:
            adjustment += self.CONFIDENCE_FACTORS['is_product_review']
        
        if context_result.has_valid_reason:
            adjustment += self.CONFIDENCE_FACTORS['has_valid_reason']
        
        if context_result.intent == ContentIntent.QUESTION:
            adjustment += self.CONFIDENCE_FACTORS['is_question']
        
        if len(text.split()) >= 20:
            adjustment += self.CONFIDENCE_FACTORS['long_context']
        
        if context_result.severity_modifier < 0.7:
            adjustment += self.CONFIDENCE_FACTORS['has_safe_context']
        
        # Apply adjustment
        calibrated = base_confidence + adjustment
        
        # Clamp to valid range
        return max(0.0, min(1.0, calibrated))


# ==================== MAIN ENHANCED ANALYZER ====================

class EnhancedModerationAnalyzer:
    """
    Advanced moderation analysis combining all components
    """
    
    def __init__(self):
        self.context_analyzer = ContextAnalyzer()
        self.semantic_checker = SemanticChecker()
        self.confidence_calibrator = ConfidenceCalibrator()
    
    def analyze(
        self,
        text: str,
        flagged_words: List[str] = None,
        base_confidence: float = 0.5
    ) -> Dict[str, Any]:
        """
        Comprehensive text analysis
        
        Args:
            text: VƒÉn b·∫£n c·∫ßn ki·ªÉm duy·ªát
            flagged_words: Danh s√°ch t·ª´ ƒë√£ b·ªã flag
            base_confidence: Confidence ban ƒë·∫ßu t·ª´ ML model
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£ ph√¢n t√≠ch chi ti·∫øt
        """
        if flagged_words is None:
            flagged_words = []
        
        # 1. Context analysis
        context_result = self.context_analyzer.analyze(text, flagged_words)
        
        # 2. Semantic check
        semantic_result = self.semantic_checker.check(text)
        
        # 3. Filter out safe context words
        safe_words = []
        actual_flagged = []
        for word in flagged_words:
            if self.context_analyzer.is_safe_context(text, word):
                safe_words.append(word)
            else:
                actual_flagged.append(word)
        
        # 4. Calibrate confidence
        calibrated_confidence = self.confidence_calibrator.calibrate(
            base_confidence,
            text,
            context_result,
            len(actual_flagged)
        )
        
        # 5. Determine final action
        if context_result.intent == ContentIntent.HATE_SPEECH:
            action = 'reject'
            reasoning = 'üö´ HATE SPEECH detected'
        elif context_result.is_legitimate_criticism and len(actual_flagged) == 0:
            action = 'allowed'
            reasoning = '‚úÖ Valid feedback/criticism'
        elif calibrated_confidence >= 0.8 and len(actual_flagged) > 0:
            action = 'reject'
            reasoning = f'‚õî Serious violation: {", ".join(actual_flagged[:3])}'
        elif calibrated_confidence >= 0.5 and len(actual_flagged) > 0:
            action = 'review'
            reasoning = f'‚ö†Ô∏è Review needed: {", ".join(actual_flagged[:3])}'
        else:
            action = 'allowed'
            reasoning = '‚úÖ Acceptable content'
        
        return {
            'action': action,
            'confidence': calibrated_confidence,
            'reasoning': reasoning,
            
            # Context details
            'intent': context_result.intent.value,
            'is_legitimate_criticism': context_result.is_legitimate_criticism,
            'targets_product': context_result.targets_product,
            'targets_person': context_result.targets_person,
            'severity_modifier': context_result.severity_modifier,
            
            # Flagged words details
            'flagged_words': actual_flagged,
            'safe_context_words': safe_words,
            
            # Semantic details
            'semantic_variations': semantic_result.get('spelling_variations', []),
            'detected_synonyms': semantic_result.get('detected_synonyms', []),
            
            # Raw context reasoning
            'context_reasoning': context_result.reasoning
        }


# Singleton instance
_analyzer_instance = None

def get_enhanced_analyzer() -> EnhancedModerationAnalyzer:
    """Get singleton instance of enhanced analyzer"""
    global _analyzer_instance
    if _analyzer_instance is None:
        _analyzer_instance = EnhancedModerationAnalyzer()
    return _analyzer_instance


if __name__ == "__main__":
    # Test
    logging.basicConfig(level=logging.DEBUG)
    
    analyzer = EnhancedModerationAnalyzer()
    
    test_cases = [
        # Legitimate negative feedback
        "S·∫£n ph·∫©m t·ªá qu√°, ch·∫•t l∆∞·ª£ng k√©m, kh√¥ng ƒë√°ng ti·ªÅn",
        "H√†ng giao ch·∫≠m, ƒë√≥ng g√≥i kh√¥ng c·∫©n th·∫≠n, th·∫•t v·ªçng",
        "Shop n√†y d·ªãch v·ª• k√©m, kh√¥ng recommend",
        
        # Toxic but product-related
        "ƒê·ªì r√°c vl, shop ngu th·∫≠t",
        
        # Personal attack
        "M√†y ngu th·∫ø, th·∫±ng n√†y kh√πng qu√°",
        
        # Hate speech
        "B·ªçn gay ƒë√°ng gh√©t, n√™n ch·∫øt h·∫øt",
        
        # Safe context
        "H√†i l√≤ng v·ªõi s·∫£n ph·∫©m",
        "C√°c b·∫°n ∆°i, s·∫£n ph·∫©m n√†y t·ªët kh√¥ng?",
        "Du l·ªãch vui qu√°",
        
        # Spam
        "Inbox nh·∫≠n ngay, zalo 0123456789, gi·∫£m gi√° 50%",
    ]
    
    print("=" * 80)
    print("ENHANCED MODERATION ANALYZER TEST")
    print("=" * 80)
    
    for text in test_cases:
        print(f"\nüìù Text: {text}")
        result = analyzer.analyze(text, flagged_words=[])
        print(f"   Action: {result['action']}")
        print(f"   Confidence: {result['confidence']:.2%}")
        print(f"   Intent: {result['intent']}")
        print(f"   Reasoning: {result['reasoning']}")
        if result['is_legitimate_criticism']:
            print(f"   ‚úÖ Legitimate criticism")
        print("-" * 60)
