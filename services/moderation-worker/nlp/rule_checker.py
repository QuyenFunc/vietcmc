"""
Layer B: Enhanced Rule-Based / Lexicon Check
=============================================

Fast, cheap, and catches obvious violations with certainty.
This is the "guardrail" layer that:
1. Catches profanity + obfuscated variants
2. Reduces load on ML model
3. Never lets obvious toxicity slip through

Key improvements:
- Uses multiple text versions from Layer A
- Adds HARASSMENT/BODY-SHAMING patterns (non-profane but harmful)
- Adds HATE SPEECH patterns for racial discrimination
- Context-aware flagging (only flag insults when targeting a person)

Version: 2.0.0
Last Updated: 2026-01-30
"""

import re
from typing import Dict, List, Tuple, Optional, Set, Any
import logging

logger = logging.getLogger(__name__)


# ==================== STEM-BASED TOXIC PATTERNS ====================
# Using regex stems to catch variations

# Core profanity stems (Vietnamese)
PROFANITY_STEMS = {
    # ƒê·ª§/ƒê·ªäT family
    'dit': {
        'patterns': [
            r'\bƒë[·ªãi√¨√≠·ªâƒ©]t\b',
            r'\bd[·ªãi√¨√≠·ªâƒ©]t\b',
            r'\bdjt\b',
            r'\bƒëjt\b',
            r'\bd1t\b',
            r'\bƒë1t\b',
            r'\bd!t\b',
            r'\bƒë!t\b',
            # ƒë·ªãt m·∫π/m√° patterns
            r'\bƒë[·ªãi√¨√≠·ªâƒ©]t\s+m[·∫πe√®√©·∫ª·∫Ω·∫π]',  # ƒë·ªãt m·∫π
            r'\bƒë[·ªãi√¨√≠·ªâƒ©]t\s+m[√°a√†·∫£√£·∫°]',   # ƒë·ªãt m√°
        ],
        'stripped_pattern': r'\bdit\b',  # For no-diacritics version
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
    },
    
    # ƒêM/DCM family  
    'dm': {
        'patterns': [
            r'\bƒëm+\b',
            r'\bdm+\b',
            r'\bƒëcm+\b',
            r'\bdcm+\b',
            r'\bƒëkm+\b',
            r'\bdkm+\b',
            r'\bƒë[·ª•u]\s*m[√°a·∫πe]',  # ƒë·ª• m√°, ƒë·ª• m·∫π
            r'\bd[·ªãi]t\s*m[√°a·∫πe]', # ƒë·ªãt m√°, ƒë·ªãt m·∫π
        ],
        'stripped_pattern': r'\bdm+\b',
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
    },
    
    # L·ªíN family
    'lon': {
        'patterns': [
            r'\bl[·ªì√¥o√≤√≥·ªè√µ·ªç]n\b',
            r'\bl0n\b',
            r'\b1on\b',
            r'\b10n\b',
        ],
        'stripped_pattern': r'\blon\b',
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
        # Safe contexts where "lon" is OK
        'safe_contexts': [
            'lon bia', 'bia lon', 'lon n∆∞·ªõc', 'n∆∞·ªõc lon',
            'lon coca', 'lon pepsi', 'lon 7up', 'lon redbull',
            'h√†i l√≤ng', 'vui l√≤ng', 'l√≤ng tin', 'l√≤ng t·ªët',
            't·∫•m l√≤ng', 'to√†n l√≤ng', 'xin l√≤ng',
        ],
    },
    
    # C·∫∂C family
    'cac': {
        'patterns': [
            r'\bc[·∫∑ƒÉ·∫Ø·∫±·∫≥·∫µ·∫°a]c\b',
            r'\bc@c\b',
            r'\bc4c\b',
            r'\bkac\b',
            r'\bk[·∫∑ƒÉa]c\b',
        ],
        'stripped_pattern': r'\bcac\b',
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
        'safe_contexts': [
            'c√°c b·∫°n', 'c√°c anh', 'c√°c ch·ªã', 'c√°c em', 'c√°c b√°c',
            'c√°c √¥ng', 'c√°c b√†', 'c√°c ch√°u', 'c√°c con',
            'm·ªôt c√°ch', 'b·∫±ng c√°ch', 'theo c√°ch', 'c√≥ c√°ch',
            'c√°c lo·∫°i', 'c√°c ki·ªÉu', 'c√°c d·∫°ng',
        ],
    },
    
    # VCL/VL family
    'vcl': {
        'patterns': [
            r'\bvcl\b',
            r'\bvkl\b',
            r'\bvl\b',
            r'\bv√£i\s*l[·ªì√¥o]n',
            r'\bvai\s*lon\b',
            r'\bv·ªù\s*c·ªù\s*l·ªù\b',
        ],
        'stripped_pattern': r'\b(vcl|vkl|vl)\b',
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
    },
    
    # CC family (con/c√°i c·∫∑c)
    'cc': {
        'patterns': [
            r'\bcc\b',
            r'\bc·ªù\s*c·ªù\b',
        ],
        'stripped_pattern': r'\bcc\b',
        'severity': 'moderate',
        'labels': ['toxicity', 'profanity'],
    },
    
    # CLM/CTM family
    'clm': {
        'patterns': [
            r'\bclm\b',
            r'\bctm\b',
            r'\bcmm\b',
        ],
        'stripped_pattern': r'\b(clm|ctm|cmm)\b',
        'severity': 'severe',
        'labels': ['toxicity', 'profanity'],
    },
    
    # NGU family (context-dependent)
    'ngu': {
        'patterns': [
            r'\bngu\s+(nh∆∞|th·∫ø|th√≠|v·∫≠y|qu√°|v√£i|vcl|vl|vkl)',
            r'\bngu\s+ng·ªëc\b',
            r'\bngu\s+si\b',
            r'\bngu\s+xu·∫©n\b',
        ],
        'stripped_pattern': r'\bngu\s+(nhu|the|thi|vay|qua|ngoc|si|xuan)\b',
        'severity': 'moderate',
        'labels': ['insult'],
        'safe_contexts': [
            'ng·ªß', 'ngu·ªìn', 'ng∆∞·ªùi', 'nguy√™n', 'nguy·ªÖn',
            'ngu·ªôi', 'ng∆∞·ªõc', 'ng·ª±a', 'ng·ª©a', 'ng∆∞ d√¢n',
        ],
        'context_required': True,  # Must match full pattern, not just "ngu"
    },
    
    # Brain/Head insults (standalone patterns)
    'brain_insults': {
        'patterns': [
            r'\bn√£o\s+(l·ª£n|ch√≥|b√≤|g√†|c√°\s*v√†ng|g·ªëi|ƒë·∫•t)\b',
            r'\b√≥c\s+(l·ª£n|ch√≥|b√≤|g√†|c√°\s*v√†ng|g·ªëi|ƒë·∫•t|chim)\b',
            r'\bƒë·∫ßu\s+(l·ª£n|ch√≥|b√≤|g√†|g·ªëi|ƒë·∫•t|b√≤|c√°)\b',
        ],
        'severity': 'moderate',
        'labels': ['insult'],
    },
    
    # Standalone insults (only flag when obfuscated)
    'obfuscated_insults': {
        'patterns': [
            # These are flagged ONLY when obfuscation is detected
            # Normal "ngu" standalone is not flagged
            # But "n.g.u" or "n-g-u" signals intentional bypass
        ],
        'standalone_words': ['ngu', 'ng·ªëc', 'ƒëi√™n', 'kh√πng', 'd·ªü'],  # Special handling
        'severity': 'moderate',
        'labels': ['insult', 'obfuscation_bypass'],
        'only_when_obfuscated': True,  # Key flag
    },
}


# ==================== HARASSMENT / BODY-SHAMING ====================
# These are NOT profane but still harmful when targeting a person

HARASSMENT_PATTERNS = {
    # Body-shaming / Appearance attacks
    'appearance_attack': {
        'patterns': [
            # Direct insults about appearance
            r'\b(m√†y|mi|n√≥|ƒë·ª©a\s*n√†y|th·∫±ng\s*n√†y|con\s*n√†y)\s+(x·∫•u|x√≠|b·∫©n|gh√™|kinh|t·ªüm|g·ªõm)',
            r'\b(m·∫∑t|da|ng∆∞·ªùi|th√¢n|body)\s+(m√†y|mi|n√≥)\s+(x·∫•u|b·∫©n|gh√™|kinh)',
            r'\b(x·∫•u|x√≠|b·∫©n|gh√™|kinh|t·ªüm)\s+(qu√°|th·∫ø|v·∫≠y|qu√°\s*tr·ªùi|v√£i)',
            
            # "nh√¨n m·∫∑t m√†y... mu·ªën n√¥n" pattern
            r'\bnh√¨n\s+(m·∫∑t|m√†y|mi|n√≥).*?(mu·ªën\s*n√¥n|gh√™\s*t·ªüm|kinh\s*t·ªüm|·ªõn|gh√©t)',
            
            # "m√†y/mi x·∫•u..." direct attack
            r'\b(sao\s+)?(m√†y|mi|n√≥)\s+(x·∫•u|x√≠|b·∫©n|h√¥i|th·ªëi|d∆°)',
        ],
        'severity': 'moderate',
        'labels': ['harassment', 'body_shaming'],
        'requires_target': True,  # Must target a person (m√†y/mi/n√≥)
    },
    
    # Personal attack indicators
    'personal_attack': {
        'patterns': [
            # "ƒë·ªì X" pattern (ƒë·ªì ngu, ƒë·ªì kh·ªën, ƒë·ªì ch√≥...)
            r'\bƒë·ªì\s+(ngu|ng·ªëc|kh·ªën|ch√≥|l·ª£n|b√≤|s√∫c\s*v·∫≠t|r√°c|v√¥\s*d·ª•ng|h√®n)',
            
            # "th·∫±ng/con X" pattern
            r'\b(th·∫±ng|con)\s+(ngu|ng·ªëc|kh·ªën|ch√≥|l·ª£n|ƒëi√™n|kh√πng|r·ªì|d·ªü)',
            
            # "th·∫±ng/con n√†y ngu" pattern
            r'\b(th·∫±ng|con)\s+(n√†y|ƒë√≥|kia)\s+(ngu|ng·ªëc|kh·ªën|ƒëi√™n)',
            
            # "m√†y l√† ƒë·ªì X"
            r'\b(m√†y|mi|n√≥)\s+(l√†\s+)?(ƒë·ªì|th·∫±ng|con)\s+(ngu|ng·ªëc|kh·ªën|ch√≥)',
        ],
        'severity': 'moderate',
        'labels': ['harassment', 'insult'],
        'requires_target': False,  # These patterns inherently indicate targeting
    },
    
    # Contempt expressions
    'contempt': {
        'patterns': [
            r'\b(gh√©t|khinh|t·ªüm|g·ªõm|·ªõn|ch√°n)\s+(m√†y|mi|n√≥|b·ªçn\s*n√†y)',
            r'\b(m√†y|mi|n√≥).*?(ƒë√°ng\s*khinh|ƒë√°ng\s*gh√©t|ƒë√°ng\s*ch·∫øt)',
            r'\b(v√¥\s*d·ª•ng|v√¥\s*gi√°\s*tr·ªã|kh√¥ng\s*ra\s*g√¨)\s*$',
        ],
        'severity': 'moderate',
        'labels': ['harassment'],
        'requires_target': True,
    },
}


# ==================== HATE SPEECH PATTERNS ====================
# Discrimination based on race, ethnicity, nationality

HATE_SPEECH_PATTERNS = {
    # Racial discrimination
    'racism': {
        'patterns': [
            # Anti-black
            r'\b(b·ªçn|l≈©|ƒë√°m|th·∫±ng|con)\s*(da\s*ƒëen|ƒëen|m·ªçi)\b',
            r'\b(da\s*ƒëen|ng∆∞·ªùi\s*ƒëen).*?(b·∫©n|th·ªëi|x·∫•u|gh√™|c√∫t|v·ªÅ\s*n∆∞·ªõc)',
            r'\b(c√∫t|bi·∫øn|ƒëi\s*ch·ªó\s*kh√°c|v·ªÅ\s*n∆∞·ªõc).*?(da\s*ƒëen|ƒëen)',
            r'\bkh·ªâ\s*ƒëen\b',
            r'\bm·ªçi\s*ƒëen\b',
            
            # Anti-Chinese
            r'\b(b·ªçn|l≈©|ƒë√°m|th·∫±ng)\s*t√†u\s*(kh·ª±a|c·ªông|gi·∫∑c)?\b',
            r'\bt√†u\s*(kh·ª±a|c·ªông|gi·∫∑c)\b',
            r'\b(chink|ching\s*chong)\b',
            
            # Anti-minority
            r'\b(b·ªçn|l≈©|ƒë√°m)\s*(m·ªçi|th·ªï\s*d√¢n|r·ª´ng\s*n√∫i)\b',
            r'\b(d√¢n\s*t·ªôc|mi·ªÅn\s*n√∫i).*?(ngu|d·ªët|l·∫°c\s*h·∫≠u|b·∫©n)',
        ],
        'severity': 'severe',
        'labels': ['hate', 'racism'],
    },
    
    # LGBTQ+ discrimination
    'lgbtq_hate': {
        'patterns': [
            r'\b(ƒë·ªì|th·∫±ng|con|b·ªçn)\s*(gay|ƒë·ªìng\s*t√≠nh|p√™\s*ƒë√™|b√™\s*ƒë√™|les)',
            r'\b(gay|ƒë·ªìng\s*t√≠nh).*?(b·ªánh|ƒë√°ng\s*ch·∫øt|t·ªüm|gh√™|kinh)',
            r'\b(ti√™u\s*di·ªát|gi·∫øt|ƒë√°nh)\s*(gay|ƒë·ªìng\s*t√≠nh|p√™\s*ƒë√™)',
        ],
        'severity': 'severe',
        'labels': ['hate', 'lgbtq_discrimination'],
    },
    
    # Xenophobia
    'xenophobia': {
        'patterns': [
            r'\b(c√∫t|bi·∫øn|ƒëi|v·ªÅ)\s*(v·ªÅ\s*n∆∞·ªõc|ƒëi\s*ch·ªó\s*kh√°c|kh·ªèi\s*ƒë√¢y)',
            r'\b(ngo·∫°i\s*qu·ªëc|ng∆∞·ªùi\s*n∆∞·ªõc\s*ngo√†i|d√¢n\s*nh·∫≠p\s*c∆∞).*?(c√∫t|bi·∫øn|v·ªÅ|b·∫©n)',
            # "bi·∫øn ƒëi (ng∆∞·ªùi n∆∞·ªõc ngo√†i/ngo·∫°i qu·ªëc)"
            r'\b(bi·∫øn|c√∫t)\s+(ƒëi\s+)?(ng∆∞·ªùi\s*n∆∞·ªõc\s*ngo√†i|ngo·∫°i\s*qu·ªëc|d√¢n\s*nh·∫≠p\s*c∆∞)',
        ],
        'severity': 'moderate',
        'labels': ['hate', 'xenophobia'],
        # NOTE: Removed additional_context - these patterns are already specific
    },
}


# ==================== PERSONAL PRONOUNS (targeting indicators) ====================

PERSONAL_ATTACK_INDICATORS = {
    # Second person pronouns (targeting someone)
    'target_pronouns': ['m√†y', 'mi', 'ng∆∞∆°i', 'bay', 'ch√∫ng m√†y', 't·ª•i m√†y', 'b·ªçn m√†y'],
    
    # Third person (talking about someone)
    'third_person': ['n√≥', 'th·∫±ng n√†y', 'con n√†y', 'ƒë·ª©a n√†y', 'th·∫±ng kia', 'con kia'],
    
    # First person (speaker)
    'speaker_pronouns': ['tao', 'tau', 'tui', 't·ªõ'],
}


# ==================== SAFE WORDS / WHITELIST ====================

# Words that should never trigger detection even if containing toxic substrings
GLOBAL_SAFE_WORDS = {
    # Common Vietnamese words
    'c√°c', 'c√°ch', 'c·ª•c', 'lon', 'l√≤ng', 'ng∆∞·ªùi', 'nh·ªØng', 'ngu·ªìn', 'ng·ªß',
    'nguy√™n', 'nguy·ªÖn', 'duy√™n', 'duy·ªát', 'du l·ªãch', 'du h·ªçc', 'gi√°o d·ª•c',
    's·ª≠ d·ª•ng', '·ª©ng d·ª•ng', 'd·ª± √°n', 'd·ªØ li·ªáu',
    
    # Product review context
    's·∫£n ph·∫©m', 'd·ªãch v·ª•', 'ch·∫•t l∆∞·ª£ng', 'giao h√†ng', 'ƒë√≥ng g√≥i',
    'shop', 'c·ª≠a h√†ng', 'ƒë√°nh gi√°', 'review',
    
    # Edit/Credit/Reddit
    'edit', 'credit', 'reddit', 'editor',
}


# ==================== MAIN CHECKER CLASS ====================

class EnhancedRuleChecker:
    """
    Enhanced rule-based / lexicon checker (Layer B)
    
    Uses multiple text versions from Layer A for comprehensive detection.
    """
    
    def __init__(self):
        # Compile all patterns
        self._compile_patterns()
    
    def _compile_patterns(self):
        """Compile regex patterns for performance"""
        self.compiled_profanity = {}
        for key, info in PROFANITY_STEMS.items():
            self.compiled_profanity[key] = {
                'patterns': [re.compile(p, re.IGNORECASE | re.UNICODE) for p in info['patterns']],
                'stripped': re.compile(info['stripped_pattern'], re.IGNORECASE) if 'stripped_pattern' in info else None,
                'info': info,
            }
        
        self.compiled_harassment = {}
        for key, info in HARASSMENT_PATTERNS.items():
            self.compiled_harassment[key] = {
                'patterns': [re.compile(p, re.IGNORECASE | re.UNICODE) for p in info['patterns']],
                'info': info,
            }
        
        self.compiled_hate = {}
        for key, info in HATE_SPEECH_PATTERNS.items():
            self.compiled_hate[key] = {
                'patterns': [re.compile(p, re.IGNORECASE | re.UNICODE) for p in info['patterns']],
                'info': info,
            }
    
    def _has_target_pronoun(self, text: str) -> bool:
        """Check if text contains pronouns indicating target (m√†y/mi/n√≥...)"""
        text_lower = text.lower()
        
        for pronoun in PERSONAL_ATTACK_INDICATORS['target_pronouns']:
            if pronoun in text_lower:
                return True
        
        for pronoun in PERSONAL_ATTACK_INDICATORS['third_person']:
            if pronoun in text_lower:
                return True
        
        return False
    
    def _is_in_safe_context(self, text: str, word: str, safe_contexts: List[str]) -> bool:
        """Check if word appears in a safe context"""
        text_lower = text.lower()
        
        for context in safe_contexts:
            if context in text_lower:
                return True
        
        return False
    
    def _check_profanity(self, text: str, text_no_diacritics: str) -> List[Dict]:
        """Check for profanity patterns"""
        findings = []
        text_lower = text.lower()
        
        for key, compiled in self.compiled_profanity.items():
            info = compiled['info']
            
            # Check safe contexts
            safe_contexts = info.get('safe_contexts', [])
            if safe_contexts and self._is_in_safe_context(text, key, safe_contexts):
                continue
            
            # Check if context required (like "ngu" needs full pattern)
            if info.get('context_required'):
                # Only match full patterns, not standalone
                for pattern in compiled['patterns']:
                    match = pattern.search(text_lower)
                    if match:
                        findings.append({
                            'type': 'profanity',
                            'key': key,
                            'matched': match.group(),
                            'severity': info['severity'],
                            'labels': info['labels'],
                        })
                        break
            else:
                # Check main patterns
                for pattern in compiled['patterns']:
                    match = pattern.search(text_lower)
                    if match:
                        findings.append({
                            'type': 'profanity',
                            'key': key,
                            'matched': match.group(),
                            'severity': info['severity'],
                            'labels': info['labels'],
                        })
                        break
                
                # Also check stripped pattern on no-diacritics version
                if not findings or findings[-1]['key'] != key:
                    if compiled['stripped']:
                        match = compiled['stripped'].search(text_no_diacritics)
                        if match:
                            # Double-check not in safe context
                            if not self._is_in_safe_context(text, key, safe_contexts):
                                findings.append({
                                    'type': 'profanity',
                                    'key': key,
                                    'matched': match.group(),
                                    'severity': info['severity'],
                                    'labels': info['labels'],
                                    'from_stripped': True,
                                })
        
        return findings
    
    def _check_harassment(self, text: str) -> List[Dict]:
        """Check for harassment/body-shaming patterns"""
        findings = []
        text_lower = text.lower()
        
        for key, compiled in self.compiled_harassment.items():
            info = compiled['info']
            
            # Check if requires target
            if info.get('requires_target') and not self._has_target_pronoun(text):
                continue
            
            for pattern in compiled['patterns']:
                match = pattern.search(text_lower)
                if match:
                    findings.append({
                        'type': 'harassment',
                        'key': key,
                        'matched': match.group(),
                        'severity': info['severity'],
                        'labels': info['labels'],
                    })
                    break
        
        return findings
    
    def _check_hate_speech(self, text: str) -> List[Dict]:
        """Check for hate speech patterns"""
        findings = []
        text_lower = text.lower()
        
        for key, compiled in self.compiled_hate.items():
            info = compiled['info']
            
            # Check additional context requirement
            additional_context = info.get('additional_context', [])
            if additional_context:
                has_context = any(ctx in text_lower for ctx in additional_context)
                if not has_context:
                    continue
            
            for pattern in compiled['patterns']:
                match = pattern.search(text_lower)
                if match:
                    findings.append({
                        'type': 'hate_speech',
                        'key': key,
                        'matched': match.group(),
                        'severity': info['severity'],
                        'labels': info['labels'],
                    })
                    break
        
        return findings
    
    def check(
        self, 
        text: str, 
        normalized_text: str = None, 
        no_diacritics_text: str = None,
        metadata: Dict = None
    ) -> Optional[Dict[str, Any]]:
        """
        Main check method.
        
        Args:
            text: Original text
            normalized_text: Fully normalized text from Layer A
            no_diacritics_text: Text with Vietnamese diacritics removed
            metadata: Normalization metadata from Layer A
        
        Returns:
            Result dict if violation found, None if clean
        """
        # Use original if normalized not provided
        if normalized_text is None:
            normalized_text = text.lower()
        if no_diacritics_text is None:
            no_diacritics_text = text.lower()
        
        all_findings = []
        
        # Check all categories
        profanity = self._check_profanity(normalized_text, no_diacritics_text)
        all_findings.extend(profanity)
        
        harassment = self._check_harassment(text)  # Use original for pronoun checking
        all_findings.extend(harassment)
        
        hate = self._check_hate_speech(text)  # Use original for full context
        all_findings.extend(hate)
        
        # Special check: obfuscated insults
        # If obfuscation was detected and normalized text contains insult words,
        # this indicates intentional bypass attempt
        if metadata and metadata.get('has_obfuscation'):
            obfuscated_insults_info = PROFANITY_STEMS.get('obfuscated_insults', {})
            standalone_words = obfuscated_insults_info.get('standalone_words', [])
            
            for word in standalone_words:
                # Check if normalized text contains this word as standalone
                if re.search(rf'\b{word}\b', normalized_text, re.IGNORECASE):
                    # Check if original text didn't contain it (meaning it was obfuscated)
                    if not re.search(rf'\b{word}\b', text.lower(), re.IGNORECASE):
                        all_findings.append({
                            'type': 'obfuscated_insult',
                            'key': 'obfuscated_insults',
                            'matched': word,
                            'severity': 'moderate',
                            'labels': ['insult', 'obfuscation_bypass'],
                        })
                        break
        
        if not all_findings:
            return None
        
        # Determine overall severity and action
        has_severe = any(f['severity'] == 'severe' for f in all_findings)
        has_hate = any(f['type'] == 'hate_speech' for f in all_findings)
        has_harassment = any(f['type'] == 'harassment' for f in all_findings)
        has_body_shaming = 'body_shaming' in [l for f in all_findings for l in f.get('labels', [])]
        
        # NEW: Escalation logic for body-shaming
        # Escalate to reject if severe expressions are used
        escalate_body_shaming = False
        if has_body_shaming or has_harassment:
            text_lower = text.lower() if 'text' in dir() else normalized_text
            severe_expressions = [
                'mu·ªën n√¥n', 'gh√™ t·ªüm', 'kinh t·ªüm', 'kinh kh·ªßng', 'gh√™ gh·ªõm',
                'ƒë√°ng ch·∫øt', 'ch·∫øt ƒëi', 'bi·∫øn ƒëi', 'c√∫t ƒëi',
                'x·∫•u kinh', 'x·∫•u gh√™', 'x·∫•u t·ªüm', 'x·∫•u kh·ªßng',
                'b√©o nh∆∞ l·ª£n', 'g·∫ßy nh∆∞ que', 'ƒëen nh∆∞ than',
                'm·∫∑t nh∆∞ l*', 'm·∫∑t l*', 'm·∫∑t nh∆∞ ƒë√≠t',
            ]
            for expr in severe_expressions:
                if expr in text_lower:
                    escalate_body_shaming = True
                    break
        
        # Collect all labels
        all_labels = set()
        for f in all_findings:
            all_labels.update(f['labels'])
        
        # Determine action
        if has_hate or has_severe or escalate_body_shaming:
            action = 'reject'
            confidence = 0.95
        else:
            action = 'review'
            confidence = 0.80
        
        # Build reasoning
        matched_items = [f['matched'] for f in all_findings[:3]]
        types = set(f['type'] for f in all_findings)
        
        reasoning_parts = []
        if 'hate_speech' in types:
            reasoning_parts.append('üö´ HATE SPEECH')
        if 'harassment' in types or 'obfuscated_insult' in types:
            if escalate_body_shaming:
                reasoning_parts.append('üö´ SEVERE HARASSMENT')
            else:
                reasoning_parts.append('‚ö†Ô∏è HARASSMENT')
        if 'profanity' in types:
            reasoning_parts.append('‚ö†Ô∏è PROFANITY')
        
        reasoning = f"{', '.join(reasoning_parts)}: {', '.join(matched_items)}"
        
        # Add obfuscation note if detected
        if metadata and metadata.get('has_obfuscation'):
            reasoning += f" (obfuscation: {', '.join(metadata['obfuscation_types'])})"
        
        return {
            'action': action,
            'labels': list(all_labels),
            'confidence': confidence,
            'reasoning': reasoning,
            'findings': all_findings,
            'method': 'rule_based_enhanced',
            'has_obfuscation': metadata.get('has_obfuscation', False) if metadata else False,
            'escalated': escalate_body_shaming,
        }


# ==================== SINGLETON INSTANCE ====================

_checker_instance = None

def get_rule_checker() -> EnhancedRuleChecker:
    """Get singleton checker instance"""
    global _checker_instance
    if _checker_instance is None:
        _checker_instance = EnhancedRuleChecker()
    return _checker_instance


# ==================== TEST ====================

if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    
    # Import normalizer
    try:
        from text_normalizer import get_normalizer
    except ImportError:
        from nlp.text_normalizer import get_normalizer
    
    normalizer = get_normalizer()
    checker = get_rule_checker()
    
    test_cases = [
        # Profanity
        "ƒëm m√†y",
        "vcl",
        "dm con ch√≥",
        
        # Obfuscated profanity
        "d.m",
        "ƒë.m",
        "n.g.u",
        "d:m",
        "d:m,m",
        
        # Harassment / body-shaming (key test cases from screenshots)
        "Sao m√†y x·∫•u th·∫ø, nh√¨n m·∫∑t m√†y tao mu·ªën n√¥n",
        "ƒë·ªì ngu ng·ªëc",
        "th·∫±ng n√†y ngu qu√°",
        
        # Hate speech (key test case from screenshot)
        "B·ªçn da ƒëen b·∫©n th·ªâu c√∫t v·ªÅ n∆∞·ªõc ƒëi",
        "t√†u kh·ª±a",
        
        # Safe content
        "S·∫£n ph·∫©m t·ªët qu√°",
        "Lon bia n√†y ngon",
        "C√°c b·∫°n c√≥ kh·ªèe kh√¥ng?",
        "H√†i l√≤ng v·ªõi d·ªãch v·ª•",
        
        # Edge cases
        "S·∫£n ph·∫©m t·ªá qu√°, th·∫•t v·ªçng",  # Negative but valid feedback
        "T√¥i kh√¥ng h√†i l√≤ng v·ªõi d·ªãch v·ª•",  # Valid complaint
    ]
    
    print("=" * 80)
    print("ENHANCED RULE CHECKER TEST")
    print("=" * 80)
    
    for text in test_cases:
        print(f"\nüìù Input: '{text}'")
        
        # Get normalized versions
        versions = normalizer.create_all_versions(text)
        
        # Run checker
        result = checker.check(
            text=text,
            normalized_text=versions['fully_normalized'],
            no_diacritics_text=versions['no_diacritics'],
            metadata=versions['metadata']
        )
        
        if result:
            print(f"   ‚ùå VIOLATION: {result['reasoning']}")
            print(f"   Action: {result['action']}, Labels: {result['labels']}")
            print(f"   Confidence: {result['confidence']:.2%}")
        else:
            print(f"   ‚úÖ CLEAN")
        
        print("-" * 60)
