"""
Layer A: Vietnamese Text Normalizer & Anti-Obfuscation
=====================================================

This module creates MULTIPLE versions of text for comprehensive detection:
1. NFC Normalized (clean Unicode)
2. Lowercase + whitespace normalized  
3. Repeated chars collapsed
4. Special char separators removed
5. Leet-speak mapped to letters
6. Vietnamese diacritics removed (for "dit me", "dm" patterns)

Key point: We must catch "dm / ƒë.m / d*m / d m / d–º–º" BEFORE passing to model.

Version: 1.0.0
Last Updated: 2026-01-30
"""

import re
import unicodedata
from typing import Dict, List, Tuple, Set
import logging

logger = logging.getLogger(__name__)


# ==================== UNICODE NORMALIZATION ====================

# Zero-width and invisible characters to remove
ZERO_WIDTH_CHARS = [
    '\u200b',  # Zero Width Space
    '\u200c',  # Zero Width Non-Joiner
    '\u200d',  # Zero Width Joiner
    '\u2060',  # Word Joiner
    '\ufeff',  # Zero Width No-Break Space (BOM)
    '\u00ad',  # Soft Hyphen
    '\u034f',  # Combining Grapheme Joiner
    '\u2063',  # Invisible Separator
    '\u2064',  # Invisible Plus
]

# Invisible whitespace characters to normalize
INVISIBLE_WHITESPACE = [
    '\u00a0',  # Non-breaking space
    '\u2000',  # En Quad
    '\u2001',  # Em Quad
    '\u2002',  # En Space
    '\u2003',  # Em Space
    '\u2004',  # Three-Per-Em Space
    '\u2005',  # Four-Per-Em Space
    '\u2006',  # Six-Per-Em Space
    '\u2007',  # Figure Space
    '\u2008',  # Punctuation Space
    '\u2009',  # Thin Space
    '\u200a',  # Hair Space
    '\u202f',  # Narrow No-Break Space
    '\u205f',  # Medium Mathematical Space
    '\u3000',  # Ideographic Space
]


# ==================== HOMOGLYPH / LOOKALIKE CHARS ====================

# Cyrillic lookalikes (very common bypass)
CYRILLIC_TO_LATIN = {
    '–∞': 'a', '–ê': 'A',  # Cyrillic A
    '–µ': 'e', '–ï': 'E',  # Cyrillic E
    '—ñ': 'i', '–Ü': 'I',  # Cyrillic I (Ukrainian)
    '–æ': 'o', '–û': 'O',  # Cyrillic O
    '—Ä': 'p', '–†': 'P',  # Cyrillic R
    '—Å': 'c', '–°': 'C',  # Cyrillic S
    '—É': 'y', '–£': 'Y',  # Cyrillic U
    '—Ö': 'x', '–•': 'X',  # Cyrillic Kha
    '–º': 'm', '–ú': 'M',  # Cyrillic M  ‚Üê CRITICAL for "d–º–º"
    '–Ω': 'n', '–ù': 'N',  # Cyrillic N
    '—Ç': 't', '–¢': 'T',  # Cyrillic T
    '–∫': 'k', '–ö': 'K',  # Cyrillic K
    '–≤': 'v', '–í': 'V',  # Cyrillic V
    '—å': '',             # Cyrillic Soft Sign - remove
    '—ä': '',             # Cyrillic Hard Sign - remove
}

# Greek lookalikes
GREEK_TO_LATIN = {
    'Œ±': 'a', 'Œë': 'A',  # Alpha
    'Œ≤': 'b', 'Œí': 'B',  # Beta
    'Œµ': 'e', 'Œï': 'E',  # Epsilon
    'Œ∑': 'n', 'Œó': 'H',  # Eta
    'Œπ': 'i', 'Œô': 'I',  # Iota
    'Œ∫': 'k', 'Œö': 'K',  # Kappa
    'Œº': 'm', 'Œú': 'M',  # Mu ‚Üê CRITICAL
    'ŒΩ': 'v', 'Œù': 'N',  # Nu
    'Œø': 'o', 'Œü': 'O',  # Omicron
    'œÅ': 'p', 'Œ°': 'P',  # Rho
    'œÑ': 't', 'Œ§': 'T',  # Tau
    'œÖ': 'u', 'Œ•': 'Y',  # Upsilon
    'œá': 'x', 'Œß': 'X',  # Chi
}

# Mathematical/Symbol lookalikes
MATH_TO_LATIN = {
    '‚Ñì': 'l',  # Script l
    '‚Ö∞': 'i',  # Roman numeral 1
    '‚Ö±': 'ii', # Roman numeral 2
    '√ó': 'x',  # Multiplication sign
    '‚àÇ': 'd',  # Partial derivative
    '‚àû': 'oo', # Infinity
    '‚à´': 'f',  # Integral
    '‚Ä†': 't',  # Dagger
    '‚Ä°': 't',  # Double dagger
}

# Full-width to half-width
FULLWIDTH_TO_HALFWIDTH = {chr(i + 0xff00 - 0x20): chr(i) for i in range(0x21, 0x7f)}


# ==================== LEETSPEAK / NUMBER SUBSTITUTION ====================

LEETSPEAK_MAP = {
    # Numbers to letters
    '0': 'o',
    '1': 'i',
    '2': 'z',  # Sometimes 2 = to
    '3': 'e',
    '4': 'a',
    '5': 's',
    '6': 'g',
    '7': 't',
    '8': 'b',
    '9': 'g',  # Sometimes 9 = q
    
    # Symbols to letters
    '@': 'a',
    '$': 's',
    '!': 'i',
    '|': 'i',
    '+': 't',
    '(': 'c',
    '[': 'c',
    ')': 'd',  # Sometimes
    '{': 'c',
    '}': 'd',
    '<': 'c',
    '>': 'd',
    '^': 'a',
    # '*' removed - handled as separator
    '#': 'h',
    '%': 'x',
    '~': 'n',
    '`': '',   # Remove
    '\\': 'l',
    '/': 'l',
}


# ==================== VIETNAMESE DIACRITICS ====================

# Vietnamese vowel mappings for diacritic removal
VIETNAMESE_DIACRITICS_MAP = {
    # A variants
    '√°': 'a', '√†': 'a', '·∫£': 'a', '√£': 'a', '·∫°': 'a',
    'ƒÉ': 'a', '·∫Ø': 'a', '·∫±': 'a', '·∫≥': 'a', '·∫µ': 'a', '·∫∑': 'a',
    '√¢': 'a', '·∫•': 'a', '·∫ß': 'a', '·∫©': 'a', '·∫´': 'a', '·∫≠': 'a',
    '√Å': 'A', '√Ä': 'A', '·∫¢': 'A', '√É': 'A', '·∫†': 'A',
    'ƒÇ': 'A', '·∫Æ': 'A', '·∫∞': 'A', '·∫≤': 'A', '·∫¥': 'A', '·∫∂': 'A',
    '√Ç': 'A', '·∫§': 'A', '·∫¶': 'A', '·∫®': 'A', '·∫™': 'A', '·∫¨': 'A',
    
    # E variants
    '√©': 'e', '√®': 'e', '·∫ª': 'e', '·∫Ω': 'e', '·∫π': 'e',
    '√™': 'e', '·∫ø': 'e', '·ªÅ': 'e', '·ªÉ': 'e', '·ªÖ': 'e', '·ªá': 'e',
    '√â': 'E', '√à': 'E', '·∫∫': 'E', '·∫º': 'E', '·∫∏': 'E',
    '√ä': 'E', '·∫æ': 'E', '·ªÄ': 'E', '·ªÇ': 'E', '·ªÑ': 'E', '·ªÜ': 'E',
    
    # I variants
    '√≠': 'i', '√¨': 'i', '·ªâ': 'i', 'ƒ©': 'i', '·ªã': 'i',
    '√ç': 'I', '√å': 'I', '·ªà': 'I', 'ƒ®': 'I', '·ªä': 'I',
    
    # O variants
    '√≥': 'o', '√≤': 'o', '·ªè': 'o', '√µ': 'o', '·ªç': 'o',
    '√¥': 'o', '·ªë': 'o', '·ªì': 'o', '·ªï': 'o', '·ªó': 'o', '·ªô': 'o',
    '∆°': 'o', '·ªõ': 'o', '·ªù': 'o', '·ªü': 'o', '·ª°': 'o', '·ª£': 'o',
    '√ì': 'O', '√í': 'O', '·ªé': 'O', '√ï': 'O', '·ªå': 'O',
    '√î': 'O', '·ªê': 'O', '·ªí': 'O', '·ªî': 'O', '·ªñ': 'O', '·ªò': 'O',
    '∆†': 'O', '·ªö': 'O', '·ªú': 'O', '·ªû': 'O', '·ª†': 'O', '·ª¢': 'O',
    
    # U variants
    '√∫': 'u', '√π': 'u', '·ªß': 'u', '≈©': 'u', '·ª•': 'u',
    '∆∞': 'u', '·ª©': 'u', '·ª´': 'u', '·ª≠': 'u', '·ªØ': 'u', '·ª±': 'u',
    '√ö': 'U', '√ô': 'U', '·ª¶': 'U', '≈®': 'U', '·ª§': 'U',
    '∆Ø': 'U', '·ª®': 'U', '·ª™': 'U', '·ª¨': 'U', '·ªÆ': 'U', '·ª∞': 'U',
    
    # Y variants
    '√Ω': 'y', '·ª≥': 'y', '·ª∑': 'y', '·ªπ': 'y', '·ªµ': 'y',
    '√ù': 'Y', '·ª≤': 'Y', '·ª∂': 'Y', '·ª∏': 'Y', '·ª¥': 'Y',
    
    # D variant
    'ƒë': 'd', 'ƒê': 'D',
}


# ==================== SEPARATOR CHARS ====================

# Characters commonly used to break up words
SEPARATOR_CHARS = set([
    '.', '-', '_', ' ', '*', '~', '^', "'", '"',
    '`', '|', '/', '\\', '+', '=', '#', '@',
    ':', ';', ',', '!', '?', '(', ')', '[', ']',
    '{', '}', '<', '>', '‚Ä¢', '¬∑', '¬∞', '‚ó¶', '‚óã', '‚óè',
])


# ==================== MAIN NORMALIZER CLASS ====================

class VietnameseTextNormalizer:
    """
    Creates multiple normalized versions of text for toxic content detection.
    
    This is Layer A of the 3-layer moderation system.
    """
    
    def __init__(self):
        # Build combined homoglyph map
        self.homoglyph_map = {}
        self.homoglyph_map.update(CYRILLIC_TO_LATIN)
        self.homoglyph_map.update(GREEK_TO_LATIN)
        self.homoglyph_map.update(MATH_TO_LATIN)
        self.homoglyph_map.update(FULLWIDTH_TO_HALFWIDTH)
        
        # Regex patterns for separator removal
        self._build_separator_pattern()
    
    def _build_separator_pattern(self):
        """Build regex pattern for separator characters between letters"""
        # Use a simpler approach - manually list common separators
        # These are: . - _ * ~ ^ ' " ` | / \ + = # @ : ; , ! ?
        sep_pattern = r'[.\-_*~^\'"`|/\\+=\#@:;,!?()\[\]{}<>‚Ä¢¬∑¬∞‚ó¶‚óã‚óè]'
        
        # Pattern: letter + separator(s) + letter
        # This will match: d.m, n.g.u, l-o-n, etc.
        viet_letters = r'[a-zA-Z√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë√Ä√Å·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê]'
        
        self.separator_between_letters = re.compile(
            f'({viet_letters})' + sep_pattern + f'+({viet_letters})',
            re.UNICODE
        )
    
    def normalize_unicode(self, text: str) -> str:
        """
        Step 1: Unicode NFC normalization + remove invisible chars
        """
        # NFC normalize (compose decomposed chars)
        text = unicodedata.normalize('NFC', text)
        
        # Remove zero-width characters
        for char in ZERO_WIDTH_CHARS:
            text = text.replace(char, '')
        
        # Normalize invisible whitespace to regular space
        for char in INVISIBLE_WHITESPACE:
            text = text.replace(char, ' ')
        
        return text
    
    def normalize_homoglyphs(self, text: str) -> Tuple[str, List[str]]:
        """
        Step 2: Replace lookalike Unicode characters with ASCII equivalents
        
        Returns:
            (normalized_text, list of replacements made)
        """
        result = []
        replacements = []
        
        for char in text:
            if char in self.homoglyph_map:
                replacement = self.homoglyph_map[char]
                result.append(replacement)
                if replacement:  # Don't log empty replacements
                    replacements.append(f"{char}‚Üí{replacement}")
            else:
                result.append(char)
        
        return ''.join(result), replacements
    
    def normalize_leetspeak(self, text: str) -> Tuple[str, List[str]]:
        """
        Step 3: Map common leetspeak/number substitutions to letters
        
        Returns:
            (normalized_text, list of conversions)
        """
        result = []
        conversions = []
        
        for char in text:
            if char in LEETSPEAK_MAP:
                replacement = LEETSPEAK_MAP[char]
                result.append(replacement)
                if replacement:
                    conversions.append(f"{char}‚Üí{replacement}")
            else:
                result.append(char)
        
        return ''.join(result), conversions
    
    def collapse_repeated_chars(self, text: str) -> str:
        """
        Step 4: Collapse repeated characters (3+ ‚Üí 2)
        "ƒëmmmmm" ‚Üí "ƒëmm"
        "nguuuuu" ‚Üí "nguu"
        """
        return re.sub(r'(.)\1{2,}', r'\1\1', text)
    
    def remove_separators_between_letters(self, text: str) -> Tuple[str, int]:
        """
        Step 5: Remove separator characters between SINGLE letters (obfuscation patterns)
        "ƒë.m" ‚Üí "ƒëm", "n.g.u" ‚Üí "ngu", "d:m" ‚Üí "dm"
        
        ALSO handles whitespace-separated single letters:
        "d  m" ‚Üí "dm", "n g u" ‚Üí "ngu"
        
        BUT preserves normal word boundaries:
        "s·∫£n ph·∫©m t·ªët" ‚Üí "s·∫£n ph·∫©m t·ªët" (unchanged)
        
        Returns:
            (cleaned_text, count of separators removed)
        """
        count = 0
        
        # PRE-PROCESSING: Handle excess whitespace between single letters
        # Pattern: single_letter + space(s) + single_letter  (repeated)
        # This catches: "d  m", "n g u", "d   m   m"
        viet_letter = r'[a-zA-Zƒëƒê]'
        
        # Find sequences of single letters separated by whitespace
        # Match: (single_letter)(\s+)(single_letter)
        def join_single_letters(m):
            return m.group(1) + m.group(3)
        
        # Iteratively join single letters separated by whitespace
        prev_text = None
        working_text = text
        while prev_text != working_text:
            prev_text = working_text
            # Pattern: single_letter at word boundary + spaces + single_letter at word boundary
            new_text = re.sub(
                rf'(?<![a-zA-Zƒëƒê])({viet_letter})(\s+)({viet_letter})(?![a-zA-Zƒëƒê])',
                join_single_letters,
                working_text
            )
            if new_text != working_text:
                count += 1
                working_text = new_text
        
        # Split into words
        words = working_text.split()
        result_words = []
        
        for word in words:
            # Only process words that look like obfuscation attempts:
            # - Short (2-10 chars)
            # - Contains separator chars
            # - Has letter-separator-letter pattern
            
            if len(word) <= 10 and any(c in SEPARATOR_CHARS for c in word):
                # Apply separator removal to this word only
                prev_word = None
                while prev_word != word:
                    prev_word = word
                    new_word = self.separator_between_letters.sub(r'\1\2', word)
                    if new_word != word:
                        count += 1
                        word = new_word
            
            result_words.append(word)
        
        return ' '.join(result_words), count
    
    def normalize_whitespace(self, text: str) -> str:
        """
        Step 6: Normalize all whitespace to single spaces
        """
        # Replace multiple spaces/tabs/newlines with single space
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def remove_vietnamese_diacritics(self, text: str) -> str:
        """
        Step 7: Remove Vietnamese diacritics
        "ƒë·ªãt m·∫π" ‚Üí "dit me"
        "ƒë√©o" ‚Üí "deo"
        
        WARNING: This should be used as a PARALLEL check, not replacement
        """
        result = []
        for char in text:
            if char in VIETNAMESE_DIACRITICS_MAP:
                result.append(VIETNAMESE_DIACRITICS_MAP[char])
            else:
                result.append(char)
        return ''.join(result)
    
    def create_all_versions(self, text: str) -> Dict[str, any]:
        """
        Main entry point: Create all normalized versions of text
        
        Returns dict with:
            - original: Original text
            - nfc: NFC normalized
            - lowercase: Lowercased
            - homoglyph_normalized: Lookalike chars replaced
            - leetspeak_normalized: Numbers/symbols replaced
            - collapsed: Repeated chars collapsed
            - separator_removed: Separators between letters removed
            - no_diacritics: Vietnamese diacritics removed
            - fully_normalized: All normalizations applied
            - metadata: Info about what was detected
        """
        metadata = {
            'homoglyph_replacements': [],
            'leetspeak_conversions': [],
            'separators_removed': 0,
            'has_obfuscation': False,
            'obfuscation_types': [],
        }
        
        # Step 1: Unicode NFC
        nfc = self.normalize_unicode(text)
        
        # Step 2: Lowercase
        lowercase = nfc.lower()
        
        # Step 3: Homoglyphs
        homoglyph_norm, homoglyph_reps = self.normalize_homoglyphs(lowercase)
        metadata['homoglyph_replacements'] = homoglyph_reps
        if homoglyph_reps:
            metadata['has_obfuscation'] = True
            metadata['obfuscation_types'].append('homoglyph')
        
        # Step 4: Leetspeak  
        leetspeak_norm, leetspeak_convs = self.normalize_leetspeak(homoglyph_norm)
        metadata['leetspeak_conversions'] = leetspeak_convs
        if leetspeak_convs:
            metadata['has_obfuscation'] = True
            metadata['obfuscation_types'].append('leetspeak')
        
        # Step 5: Collapse repeated
        collapsed = self.collapse_repeated_chars(leetspeak_norm)
        
        # Step 6: Remove separators
        separator_removed, sep_count = self.remove_separators_between_letters(collapsed)
        metadata['separators_removed'] = sep_count
        if sep_count > 0:
            metadata['has_obfuscation'] = True
            metadata['obfuscation_types'].append('separator_insertion')
        
        # Step 7: Normalize whitespace
        fully_normalized = self.normalize_whitespace(separator_removed)
        
        # Step 8: No diacritics version (parallel check)
        no_diacritics = self.remove_vietnamese_diacritics(fully_normalized)
        
        return {
            'original': text,
            'nfc': nfc,
            'lowercase': lowercase,
            'homoglyph_normalized': homoglyph_norm,
            'leetspeak_normalized': leetspeak_norm,
            'collapsed': collapsed,
            'separator_removed': separator_removed,
            'no_diacritics': no_diacritics,
            'fully_normalized': fully_normalized,
            'metadata': metadata,
        }
    
    def get_texts_for_checking(self, text: str) -> List[Tuple[str, str]]:
        """
        Get list of (text_version, version_name) tuples to check against rules
        
        This is the main method to use in the moderation pipeline
        """
        versions = self.create_all_versions(text)
        
        return [
            (versions['original'], 'original'),
            (versions['fully_normalized'], 'normalized'),
            (versions['no_diacritics'], 'no_diacritics'),
        ]


# ==================== SINGLETON INSTANCE ====================

_normalizer_instance = None

def get_normalizer() -> VietnameseTextNormalizer:
    """Get singleton normalizer instance"""
    global _normalizer_instance
    if _normalizer_instance is None:
        _normalizer_instance = VietnameseTextNormalizer()
    return _normalizer_instance


# ==================== TEST ====================

if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    
    normalizer = get_normalizer()
    
    test_cases = [
        # Standard
        "S·∫£n ph·∫©m t·ªët qu√°",
        
        # Separator obfuscation
        "ƒë.m m√†y",
        "d*m",
        "n.g.u",
        "d:m",
        "d:m,m",
        "l-o-n",
        "c.a.c",
        
        # Leetspeak
        "d1t me",
        "l0n",
        "c@c",
        "n9u",
        
        # Cyrillic/Greek lookalikes
        "d–º–º",  # Cyrillic –º instead of m
        "ŒΩcl",  # Greek ŒΩ instead of v
        
        # Repeated chars
        "ƒëmmmmm",
        "nguuuuu",
        
        # No diacritics
        "dit me may",
        "dm con cho",
        
        # Combined obfuscation
        "ƒë.–º.–º",  # Separator + Cyrillic
        "d*!*t",  # Multiple symbols
        
        # Zero-width characters (invisible)
        "ƒë\u200bm\u200bm",  # Zero-width spaces
        
        # Full-width characters
        "ÔΩÑÔΩç",  # Full-width dm
        
        # Spacing bypass
        "d  m",
        "ƒë   m",
    ]
    
    print("=" * 80)
    print("VIETNAMESE TEXT NORMALIZER TEST")
    print("=" * 80)
    
    for text in test_cases:
        print(f"\nüìù Input: '{text}'")
        
        # Show repr for invisible chars
        if any(ord(c) > 127 or c in '\u200b\u200c\u200d' for c in text):
            print(f"   Repr: {repr(text)}")
        
        versions = normalizer.create_all_versions(text)
        
        print(f"   Normalized: '{versions['fully_normalized']}'")
        print(f"   No diacritics: '{versions['no_diacritics']}'")
        
        meta = versions['metadata']
        if meta['has_obfuscation']:
            print(f"   ‚ö†Ô∏è OBFUSCATION DETECTED: {meta['obfuscation_types']}")
            if meta['homoglyph_replacements']:
                print(f"      Homoglyphs: {meta['homoglyph_replacements']}")
            if meta['leetspeak_conversions']:
                print(f"      Leetspeak: {meta['leetspeak_conversions']}")
            if meta['separators_removed']:
                print(f"      Separators removed: {meta['separators_removed']}")
        
        print("-" * 60)
