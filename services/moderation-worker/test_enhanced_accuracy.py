"""
Test Enhanced Moderation System
Test c√°c c·∫£i ti·∫øn v·ªÅ ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng ki·ªÉm duy·ªát

Usage:
    cd services/moderation-worker
    python test_enhanced_accuracy.py
"""

import logging
import sys
import os

# Setup path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from nlp.context_analyzer import EnhancedModerationAnalyzer, get_enhanced_analyzer
from nlp.inference_multitask import MultiTaskModerationInference

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def test_context_analyzer():
    """Test context analyzer independently"""
    print("\n" + "=" * 80)
    print("üîç TEST CONTEXT ANALYZER")
    print("=" * 80)
    
    analyzer = get_enhanced_analyzer()
    
    test_cases = [
        # ===== SHOULD BE ALLOWED (Legitimate negative feedback) =====
        {
            'text': 'S·∫£n ph·∫©m t·ªá qu√°, ch·∫•t l∆∞·ª£ng k√©m, kh√¥ng ƒë√°ng ti·ªÅn',
            'expected_action': 'allowed',
            'category': 'Legitimate negative feedback'
        },
        {
            'text': 'H√†ng giao ch·∫≠m, ƒë√≥ng g√≥i kh√¥ng c·∫©n th·∫≠n, th·∫•t v·ªçng',
            'expected_action': 'allowed',
            'category': 'Legitimate complaint'
        },
        {
            'text': 'Shop n√†y d·ªãch v·ª• k√©m, kh√¥ng recommend',
            'expected_action': 'allowed',
            'category': 'Legitimate recommendation'
        },
        {
            'text': 'ƒê√°nh gi√° 1 sao, s·∫£n ph·∫©m kh√¥ng nh∆∞ m√¥ t·∫£',
            'expected_action': 'allowed',
            'category': 'Star rating feedback'
        },
        {
            'text': 'H√†i l√≤ng v·ªõi s·∫£n ph·∫©m, s·∫Ω mua l·∫°i',
            'expected_action': 'allowed',
            'category': 'Positive feedback'
        },
        {
            'text': 'Du l·ªãch ƒê√† N·∫µng vui qu√°',
            'expected_action': 'allowed',
            'category': 'Travel content'
        },
        {
            'text': 'T√¨nh h√¨nh gay g·∫Øt, c·∫ßn gi·∫£i quy·∫øt',
            'expected_action': 'allowed',
            'category': 'Normal Vietnamese phrase'
        },
        {
            'text': 'C√°c b·∫°n ∆°i, s·∫£n ph·∫©m n√†y t·ªët kh√¥ng?',
            'expected_action': 'allowed',
            'category': 'Question'
        },
        
        # ===== SHOULD BE REVIEW (Borderline cases) =====
        {
            'text': 'ƒê·ªì r√°c vl, shop ngu th·∫≠t',
            'expected_action': 'review',
            'category': 'Profanity in feedback'
        },
        
        # ===== SHOULD BE REJECT (Clear violations) =====
        {
            'text': 'M√†y ngu th·∫ø, th·∫±ng n√†y kh√πng qu√°',
            'expected_action': 'reject',
            'category': 'Personal attack'
        },
        {
            'text': 'B·ªçn gay ƒë√°ng gh√©t, n√™n ch·∫øt h·∫øt',
            'expected_action': 'reject',
            'category': 'Hate speech'
        },
        {
            'text': 'ƒêm m√†y, tao gi·∫øt m√†y',
            'expected_action': 'reject',
            'category': 'Threat + profanity'
        },
    ]
    
    passed = 0
    failed = 0
    
    for case in test_cases:
        result = analyzer.analyze(case['text'], flagged_words=[])
        actual_action = result['action']
        expected = case['expected_action']
        
        status = "‚úÖ PASS" if actual_action == expected else "‚ùå FAIL"
        if actual_action == expected:
            passed += 1
        else:
            failed += 1
        
        print(f"\n{status} | {case['category']}")
        print(f"   Text: {case['text'][:60]}...")
        print(f"   Expected: {expected} | Actual: {actual_action}")
        print(f"   Confidence: {result['confidence']:.2%}")
        print(f"   Intent: {result['intent']}")
        if result['is_legitimate_criticism']:
            print(f"   ‚úì Legitimate criticism detected")
    
    print(f"\n{'='*80}")
    print(f"Results: {passed}/{passed+failed} passed ({passed/(passed+failed)*100:.1f}%)")
    print(f"{'='*80}")
    
    return passed, failed


def test_safe_context_detection():
    """Test detection of safe contexts for words that look toxic"""
    print("\n" + "=" * 80)
    print("üîç TEST SAFE CONTEXT DETECTION")
    print("=" * 80)
    
    analyzer = get_enhanced_analyzer()
    
    test_cases = [
        # Words that look toxic but are in safe context
        ('T√¨nh h√¨nh gay g·∫Øt qu√°', 'gay', True, 'gay g·∫Øt = tense'),
        ('H√†i l√≤ng v·ªõi d·ªãch v·ª•', 'lon', True, 'h√†i l√≤ng = satisfied'),
        ('Du l·ªãch Vi·ªát Nam', 'du', True, 'du l·ªãch = travel'),
        ('C√°c b·∫°n c√≥ kh·ªèe kh√¥ng', 'c√°c', True, 'c√°c = plural marker'),
        ('Lon bia l·∫°nh ngon', 'lon', True, 'lon bia = beer can'),
        
        # Words that ARE toxic (should NOT be filtered)
        ('ƒê·ªì ngu', 'ngu', False, 'ngu = stupid (toxic)'),
        ('Th·∫±ng ƒëi√™n', 'ƒëi√™n', False, 'ƒëi√™n = crazy (toxic)'),
    ]
    
    passed = 0
    for text, word, expected_safe, description in test_cases:
        is_safe = analyzer.context_analyzer.is_safe_context(text, word)
        status = "‚úÖ" if is_safe == expected_safe else "‚ùå"
        if is_safe == expected_safe:
            passed += 1
        
        print(f"{status} '{word}' in '{text[:40]}...' - Safe: {is_safe} (expected: {expected_safe})")
        print(f"   Reason: {description}")
    
    print(f"\nResults: {passed}/{len(test_cases)} passed")
    return passed, len(test_cases) - passed


def test_severity_modifier():
    """Test severity modifier calculation"""
    print("\n" + "=" * 80)
    print("üîç TEST SEVERITY MODIFIER")
    print("=" * 80)
    
    analyzer = get_enhanced_analyzer()
    
    test_cases = [
        ('S·∫£n ph·∫©m t·ªët', 'Should have modifier ‚âà 1.0'),
        ('S·∫£n ph·∫©m t·ªá qu√°', 'Should reduce severity (product feedback)'),
        ('ƒê√πa th√¥i', 'Should reduce severity (joking)'),
        ('Tao gi·∫øt m√†y', 'Should increase severity (threat)'),
        ('Kh√¥ng ph·∫£i v·∫≠y', 'Should reduce severity (negation)'),
    ]
    
    for text, description in test_cases:
        result = analyzer.context_analyzer.analyze(text)
        modifier = result.severity_modifier
        
        indicator = "üîª" if modifier < 0.9 else ("üî∫" if modifier > 1.1 else "‚ûñ")
        print(f"{indicator} Modifier: {modifier:.2f} | Text: '{text}'")
        print(f"   {description}")
    
    print("\nLegend: üîª = Reduced severity, üî∫ = Increased severity, ‚ûñ = Normal")


def test_inference_with_context():
    """Test full inference with context analyzer"""
    print("\n" + "=" * 80)
    print("üîç TEST INFERENCE WITH CONTEXT ANALYZER")
    print("=" * 80)
    
    try:
        inference = MultiTaskModerationInference(
            model_path='vinai/phobert-base-v2',
            device='cpu',
            use_context_analyzer=True
        )
        
        test_texts = [
            # Should be ALLOWED
            "S·∫£n ph·∫©m r·∫•t t·ªët, t√¥i r·∫•t h√†i l√≤ng!",
            "H√†ng giao ch·∫≠m, ch·∫•t l∆∞·ª£ng t·ªá",
            "Du l·ªãch ƒê√† N·∫µng th·∫≠t vui",
            "C√°c b·∫°n n√™n th·ª≠ s·∫£n ph·∫©m n√†y",
            
            # Should be REVIEW or REJECT
            "ƒê·ªì r√°c v√£i l·ªìn",
            "Th·∫±ng shop n√†y l·ª´a ƒë·∫£o",
            
            # Should be REJECT
            "B·ªçn kh·ªâ ƒëen n√†y ngu v√£i",
        ]
        
        for text in test_texts:
            result = inference.predict(text)
            action = result.get('action', 'unknown')
            confidence = result.get('confidence', 0)
            method = result.get('method', 'unknown')
            
            icon = "‚úÖ" if action == 'allowed' else ("‚ö†Ô∏è" if action == 'review' else "üö´")
            print(f"\n{icon} [{action.upper()}] {confidence:.1%} | {text[:50]}...")
            print(f"   Method: {method}")
            if result.get('reasoning'):
                print(f"   Reason: {result['reasoning'][:80]}...")
    
    except Exception as e:
        print(f"‚ö†Ô∏è Could not run full inference test: {e}")
        print("   (This is expected if running without full model setup)")


def main():
    print("\n" + "üöÄ" * 40)
    print("ENHANCED MODERATION ACCURACY TEST")
    print("üöÄ" * 40)
    
    # Test 1: Context Analyzer
    passed1, failed1 = test_context_analyzer()
    
    # Test 2: Safe Context Detection
    passed2, failed2 = test_safe_context_detection()
    
    # Test 3: Severity Modifier
    test_severity_modifier()
    
    # Test 4: Full Inference (optional)
    test_inference_with_context()
    
    # Summary
    total_passed = passed1 + passed2
    total_failed = failed1 + failed2
    
    print("\n" + "=" * 80)
    print("üìä FINAL SUMMARY")
    print("=" * 80)
    print(f"Total Tests: {total_passed + total_failed}")
    print(f"Passed: {total_passed}")
    print(f"Failed: {total_failed}")
    print(f"Success Rate: {total_passed/(total_passed+total_failed)*100:.1f}%")
    
    if total_failed == 0:
        print("\nüéâ All tests passed! Enhanced moderation is working correctly.")
    else:
        print(f"\n‚ö†Ô∏è {total_failed} tests failed. Please review the results above.")


if __name__ == "__main__":
    main()
